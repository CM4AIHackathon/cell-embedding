# -*- coding: utf-8 -*-
"""cleaned-up.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pkwhE6hAdVPi81a8m2ltGx7T7Rdqa6Y5
"""

# %% Cell 2: (UNCHANGED)
#!/usr/bin/env python3
"""
Targeted installation script for:
- MindSpore 2.4.0 (working with NumPy 2.x)
- scanpy 1.10
- scib 1.1.5

This script works around NumPy compatibility issues by using specific workarounds
"""

import subprocess
import sys
import importlib
import os
import time

def install_mindspore_24_with_numpy2():
    """Install MindSpore 2.4.0 with NumPy 2.x compatibility fixes"""
    print("🧠 Installing MindSpore 2.4.0 with NumPy 2.x compatibility...")

    # Check current NumPy version
    try:
        import numpy as np
        numpy_version = np.__version__
        print(f"Current NumPy version: {numpy_version}")
    except ImportError:
        print("❌ NumPy not found!")
        return False

    # Strategy 1: Install MindSpore 2.4.0 with --no-deps and handle numpy.rec issue
    print("Installing MindSpore 2.4.0 with --no-deps...")
    success, stdout, stderr = run_pip_command(["install", "mindspore==2.4.0", "--no-deps"])

    if not success:
        print(f"❌ MindSpore installation failed: {stderr}")
        return False

    print("✅ MindSpore 2.4.0 installed")

    # Install MindSpore dependencies manually
    print("Installing MindSpore dependencies...")
    dependencies = [
        "protobuf>=3.13.0,<5.0.0",
        "psutil>=5.6.1",
        "packaging>=20.0",
        "pillow>=6.2.0",
        "asttokens>=2.0.4"
    ]

    for dep in dependencies:
        success, _, stderr = run_pip_command(["install", dep])
        if success:
            print(f"✅ Installed {dep}")
        else:
            print(f"⚠️ Failed to install {dep}: {stderr[:100]}")

    # Apply NumPy 2.x compatibility fix
    print("Applying NumPy 2.x compatibility fix...")

    # Create a compatibility patch for numpy.rec
    try:
        import numpy as np
        if not hasattr(np, 'rec'):
            # Create numpy.rec compatibility
            import numpy.core.records as records
            np.rec = records
            print("✅ Applied numpy.rec compatibility patch")
    except Exception as e:
        print(f"⚠️ Could not apply numpy.rec patch: {e}")

    # Test MindSpore import
    print("Testing MindSpore import...")
    try:
        # Force reload modules
        if 'mindspore' in sys.modules:
            del sys.modules['mindspore']

        import mindspore as ms
        print(f"✅ MindSpore {ms.__version__} imported successfully!")

        # Test basic functionality
        try:
            import mindspore.numpy as mnp
            x = mnp.array([1.0, 2.0, 3.0])
            print(f"✅ MindSpore tensor creation works: {x}")
            return True
        except Exception as e:
            print(f"⚠️ MindSpore imports but tensor creation failed: {e}")
            return True  # Still consider it a success if import works

    except ImportError as e:
        print(f"❌ MindSpore import failed: {e}")

        # Try alternative approach with environment variable
        print("Trying with MINDSPORE_NUMPY_COMPATIBLE environment variable...")
        os.environ['MINDSPORE_NUMPY_COMPATIBLE'] = '1'

        try:
            import mindspore as ms
            print(f"✅ MindSpore {ms.__version__} imported with env fix!")
            return True
        except ImportError as e2:
            print(f"❌ MindSpore import still failed: {e2}")
            return False
def run_pip_command(args, capture_output=True, timeout=300):
    """Run pip command with error handling"""
    try:
        cmd = [sys.executable, "-m", "pip"] + args
        result = subprocess.run(cmd, capture_output=capture_output, text=True, timeout=timeout)
        return result.returncode == 0, result.stdout, result.stderr
    except subprocess.TimeoutExpired:
        return False, "", "Command timed out"
    except Exception as e:
        return False, "", str(e)

def install_scanpy_10():
    """Install scanpy 1.10 with dependencies"""
    print("\n📊 Installing scanpy 1.10...")

    # Install scanpy with specific version
    success, stdout, stderr = run_pip_command(["install", "scanpy==1.10.0"])

    if success:
        print("✅ scanpy 1.10.0 installed")
    else:
        print(f"❌ scanpy installation failed: {stderr}")

        # Try installing dependencies first
        print("Installing scanpy dependencies first...")
        deps = [
            "anndata>=0.8.0",
            "pandas>=1.0",
            "scipy>=1.4",
            "seaborn",
            "h5py>=3.1.0",
            "tqdm",
            "scikit-learn>=0.22",
            "statsmodels>=0.10.0rc2",
            "patsy",
            "networkx>=2.3",
            "natsort",
            "joblib",
            "numba>=0.41.0",
            "umap-learn>=0.3.10",
            "leidenalg",
            "igraph>=0.9"
        ]

        for dep in deps:
            success_dep, _, _ = run_pip_command(["install", dep])
            if success_dep:
                print(f"✅ Installed {dep}")

        # Try scanpy again
        success, stdout, stderr = run_pip_command(["install", "scanpy==1.10.0", "--no-deps"])
        if success:
            print("✅ scanpy 1.10.0 installed with --no-deps")
        else:
            print(f"❌ scanpy installation still failed: {stderr}")
            return False

    # Test scanpy import
    success, version = check_package_import("scanpy", "scanpy")
    if success:
        print(f"✅ scanpy {version} imported successfully")
        return True
    else:
        print(f"❌ scanpy import failed: {version}")
        return False

def install_scib_115():
    """Install scib 1.1.5 with dependencies"""
    print("\n🔧 Installing scib 1.1.5...")

    # First install scib dependencies manually
    print("Installing scib dependencies...")
    scib_deps = [
        "scanpy>=1.6.0",
        "anndata>=0.7.4",
        "pandas>=1.0.0",
        "seaborn",
        "scikit-learn",
        "scipy",
        "numpy",
        "matplotlib",
        "deprecated",
        "rich>=10.0.0"
    ]

    for dep in scib_deps:
        success, _, stderr = run_pip_command(["install", dep])
        if success:
            print(f"✅ Installed {dep}")
        else:
            print(f"⚠️ Failed to install {dep}")

    # Try installing scib
    success, stdout, stderr = run_pip_command(["install", "scib==1.1.5"])

    if success:
        print("✅ scib 1.1.5 installed")
    else:
        print(f"❌ scib installation failed: {stderr}")

        # Try with --no-deps
        print("Trying scib installation with --no-deps...")
        success, stdout, stderr = run_pip_command(["install", "scib==1.1.5", "--no-deps"])

        if success:
            print("✅ scib 1.1.5 installed with --no-deps")
        else:
            print(f"❌ scib installation failed even with --no-deps: {stderr}")
            return False

    # Test scib import
    success, version = check_package_import("scib", "scib")
    if success:
        print(f"✅ scib {version} imported successfully")
        return True
    else:
        print(f"❌ scib import failed: {version}")
        return False

def main():
    """Main installation function"""
    print("🎯 Targeted Installation: MindSpore 2.4.0 + scanpy 1.10 + scib 1.1.5")
    print("=" * 70)

    results = {}

    # Install MindSpore 2.4.0
    results['mindspore'] = install_mindspore_24_with_numpy2()

    # Install scanpy 1.10
    results['scanpy'] = install_scanpy_10()

    # Install scib 1.1.5
    results['scib'] = install_scib_115()

    # Final verification
    print("\n🔍 Final Verification")
    print("=" * 30)

    packages_to_test = [
        ('MindSpore', 'mindspore'),
        ('scanpy', 'scanpy'),
        ('scib', 'scib'),
        ('NumPy', 'numpy'),
        ('SciPy', 'scipy'),
        ('pandas', 'pandas'),
        ('AnnData', 'anndata')
    ]

    final_results = {}
    for name, import_name in packages_to_test:
        success, info = check_package_import(import_name)
        final_results[name] = success
        status = "✅" if success else "❌"
        print(f"{status} {name}: {info}")

    # Summary
    print(f"\n📊 SUMMARY")
    print("=" * 20)
    success_count = sum(results.values())
    total_count = len(results)

    print(f"Target packages: {success_count}/{total_count} successful")
    print(f"MindSpore 2.4.0: {'✅' if results['mindspore'] else '❌'}")
    print(f"scanpy 1.10: {'✅' if results['scanpy'] else '❌'}")
    print(f"scib 1.1.5: {'✅' if results['scib'] else '❌'}")

    if success_count == total_count:
        print("\n🎉 All target packages installed successfully!")

        # Quick functionality test
        print("\n🧪 Quick functionality test...")
        try:
            import mindspore as ms
            import scanpy as sc
            import numpy as np

            # Test data creation
            data = np.random.randn(100, 50)
            print(f"✅ Created test data: {data.shape}")

            # Test MindSpore tensor
            ms_tensor = ms.Tensor(data[:10, :10])
            print(f"✅ MindSpore tensor: {ms_tensor.shape}")

            print("🎯 All core functionality working!")

        except Exception as e:
            print(f"⚠️ Functionality test failed: {e}")

    else:
        print(f"\n⚠️ {total_count - success_count} packages failed to install properly")
        print("Check the error messages above for troubleshooting")

if __name__ == "__main__":
    main()

import numpy as np

print(np.version.version)

#!/usr/bin/env python3
"""
CellFM — ZERO-SHOT embeddings on CPU (Colab/MindSpore)
SCrna → Prepare → build_dataset → CellFM forward()
- No train/test split; shards are for compute only
- Uses ONLY official CellFM modules (no monkey patches)

Hardening:
  • NumPy 2 shim (idempotent): np.array(copy=False) → np.asarray
  • MindSpore import/context guard: helpful fix instructions if env is broken
  • Force model→fp32 on CPU (safer than fp16 ops)
  • Shards saved with metadata; reload validated or regenerate
  • Dataset column sanity-check (one-time) with shapes logged
  • Strict model-output CLS extraction with detected embed dim
  • NaN/Inf cleanup + CSR for .h5ad; SCrna points to cleaned copy
"""

# ========================= NUMPY 2 COMPAT SHIM ===============================
import numpy as np

if not getattr(np, "_cellfm_array_patched", False):
    np._cellfm_array_patched = True
    np._cellfm_array_orig = np.array  # save original exactly once

    def _cellfm_safe_array(obj, *args, **kwargs):
        # Only handle the problematic pattern np.array(..., copy=False)
        if kwargs.get("copy", None) is False:
            kwargs.pop("copy", None)        # remove incompatible perf hint
            subok = kwargs.pop("subok", None)
            # keep only kwargs that asarray/asanyarray actually support
            dtype = kwargs.pop("dtype", None)
            order = kwargs.pop("order", None)
            like  = kwargs.pop("like", None)
            if subok is True:
                return np.asanyarray(obj, dtype=dtype, order=order, like=like)
            else:
                return np.asarray(obj, dtype=dtype, order=order, like=like)
        # Otherwise, preserve exact NumPy semantics
        return np._cellfm_array_orig(obj, *args, **kwargs)

    np.array = _cellfm_safe_array
# ============================================================================

print(np.array([1, 2, 3]))  # sanity: should print [1 2 3]


# ============================== USER CONFIG ==================================
BATCH_ID   = 0          # 0..NUM_SHARDS-1
NUM_SHARDS = 5
DO_MERGE   = False

ACTUAL_MODEL_PATH   = "/content/drive/MyDrive/iPSC_CM/CellFM_80M_weight.ckpt"
ACTUAL_DATASET_H5AD = "/content/drive/MyDrive/iPSC_CM/datasets/CM4AI/KOLF2.1J_undifferentiated_untreated_chromatin.h5ad"
DRIVE_OUTPUT_PATH   = "/content/drive/MyDrive/FINAL_CellFM_Complete_Analysis_Results"

# Embedding / batching
EMBED_SEQ_CAP = 256
BATCH_SIZE    = 48
MICRO_BSZ     = 16
SEED          = 0

# ============================== ENV PREP =====================================
print("🔧 Preparing environment...")
import os, gc, shutil, warnings, subprocess, importlib, sys, h5py, pandas as pd
warnings.filterwarnings("ignore")

os.environ.setdefault("OMP_NUM_THREADS", "1")
os.environ.setdefault("MKL_NUM_THREADS", "1")

import scanpy as sc
from tqdm.auto import tqdm
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from scipy.sparse import issparse, csr_matrix

# --- MindSpore import guard (handles broken installs gracefully) -------------
def _ensure_mindspore_or_exit():
    try:
        import mindspore as ms
        from mindspore.train.serialization import load_checkpoint, load_param_into_net
        from mindspore import dtype as mstype
        ms.set_context(mode=ms.PYNATIVE_MODE, device_target="CPU")
        ms.set_seed(SEED)
        return ms, load_checkpoint, load_param_into_net, mstype
    except Exception as e:
        py = sys.version.split()[0]
        print("❌ MindSpore failed to import. This is an environment issue, not a code bug.")
        print(f"Detected Python {py}. To fix in Colab CPU, run *in a new cell*:")
        print("  pip uninstall -y mindspore mindspore-gpu mindspore-cpu mindspore-ascend mindspore-lite")
        print("  pip install --no-cache-dir "
              "https://ms-release.obs.cn-north-4.myhuaweicloud.com/2.4.10/MindSpore/unified/x86_64/"
              "mindspore-2.4.10-cp311-cp311-linux_x86_64.whl "
              "--trusted-host ms-release.obs.cn-north-4.myhuaweicloud.com")
        print("Then: Runtime → Restart, and re-run this script.")
        raise  # stop here so you see the instructions clearly

ms, load_checkpoint, load_param_into_net, mstype = _ensure_mindspore_or_exit()

# Minimize dataset queue memory/threads
try:
    import mindspore.dataset as ds
    ds.config.set_prefetch_size(1)
    ds.config.set_num_parallel_workers(1)
except Exception:
    pass

# Drive (Colab)
try:
    from google.colab import drive
    drive.mount("/content/drive", force_remount=False)
    print("✅ Drive mounted")
except Exception as e:
    print(f"ℹ️ Skipping Drive mount: {e}")

# ============================== CLONE / UPDATE ===============================
REPO_ROOT = "/content/CellFM"
if os.path.exists(REPO_ROOT):
    try:
        subprocess.run(["git", "-C", REPO_ROOT, "pull"], check=True, capture_output=True, text=True)
        print("✅ Updated CellFM repo")
    except Exception:
        print("ℹ️ Using existing CellFM repo")
else:
    subprocess.run(["git", "clone", "https://github.com/biomed-AI/CellFM.git", REPO_ROOT], check=True)
    print("✅ Cloned CellFM repo")

# Ensure gene_info.csv at repo root (symlink or copy)
gi_root = os.path.join(REPO_ROOT, "gene_info.csv")
if not os.path.exists(gi_root):
    candidates = [os.path.join(REPO_ROOT, "csv", "gene_info.csv"),
                  os.path.join(REPO_ROOT, "assets", "gene_info.csv")]
    linked = False
    for c in candidates:
        if os.path.exists(c):
            try:
                os.symlink(c, gi_root)
            except FileExistsError:
                pass
            except OSError:
                shutil.copy2(c, gi_root)
            linked = True
            break
    if linked:
        print(f"🔗 Linked gene_info.csv → {gi_root}")
    else:
        raise FileNotFoundError("gene_info.csv not found inside CellFM repo.")

print("📂 Working directory set to CellFM root for consistent relative paths")
os.chdir(REPO_ROOT)
if REPO_ROOT not in sys.path: sys.path.insert(0, REPO_ROOT)

# Import OFFICIAL CellFM modules
model_mod = importlib.import_module("model")
config_mod = importlib.import_module("config")
data_process_mod = importlib.import_module("data_process")
importlib.reload(model_mod); importlib.reload(config_mod); importlib.reload(data_process_mod)
from model import CellFM
from config import Config_80M
from data_process import Prepare, SCrna, build_dataset

# ============================== WORKSPACE PATHS ==============================
LOCAL_BASE = "/content/cellfm_cpu_pipeline"
os.makedirs(LOCAL_BASE, exist_ok=True)

CKPT_PATH         = os.path.join(LOCAL_BASE, "CellFM_80M_weight.ckpt")
RAW_DATASETPATH   = os.path.join(LOCAL_BASE, "dataset.raw.h5ad")
CLEAN_DATASETPATH = os.path.join(LOCAL_BASE, "dataset.cleaned.h5ad")
OUTDIR      = os.path.join(LOCAL_BASE, "results");  os.makedirs(OUTDIR, exist_ok=True)
FIGDIR      = os.path.join(OUTDIR, "figures");     os.makedirs(FIGDIR, exist_ok=True)
PARTS_DIR   = os.path.join(DRIVE_OUTPUT_PATH, "parts");  os.makedirs(PARTS_DIR, exist_ok=True)
SHARD_DIR   = os.path.join(DRIVE_OUTPUT_PATH, "shards"); os.makedirs(SHARD_DIR, exist_ok=True)

# Copy heavy files local (first time)
for src, dst in [(ACTUAL_MODEL_PATH, CKPT_PATH), (ACTUAL_DATASET_H5AD, RAW_DATASETPATH)]:
    if os.path.exists(src) and not os.path.exists(dst):
        print(f"📥 Copying {os.path.basename(src)} → local ...")
        shutil.copy2(src, dst)
print(f"   • Using gene_info.csv at: {gi_root}")

# ============================== HELPERS ======================================
def emergency_gc(): gc.collect()

def sanitize_and_save_h5ad(src_path, dst_path):
    """Load .h5ad, replace NaN/Inf in X with 0, ensure CSR, clean numeric obs/var, write to dst."""
    print(f"🧼 Sanitizing .h5ad → {dst_path}")
    ad = sc.read_h5ad(src_path)
    if issparse(ad.X):
        if not isinstance(ad.X, csr_matrix):
            ad.X = ad.X.tocsr()
        before_nnz = ad.X.data.size
        ad.X.data = np.nan_to_num(ad.X.data, nan=0.0, posinf=0.0, neginf=0.0)
        after_nnz = ad.X.data.size
        print(f"   • Sparse CSR with {after_nnz:,} stored values (was {before_nnz:,})")
    else:
        ad.X = np.nan_to_num(ad.X, nan=0.0, posinf=0.0, neginf=0.0)
        print(f"   • Dense matrix sanitized")
    if "gene_name" not in ad.var.columns:
        ad.var["gene_name"] = ad.var_names.astype(str)
    # Replace the problematic metadata cleaning section with this:
    for df in (ad.obs, ad.var):
        for col in df.columns:
            try:
                s = df[col]
                # Only process if it's clearly a numeric column
                if pd.api.types.is_numeric_dtype(s):
                    df[col] = np.nan_to_num(s.values, nan=0.0, posinf=0.0, neginf=0.0)
            except Exception:
                # Skip any problematic columns
                continue
    ad.write(dst_path, compression="gzip")
    n_obs, n_vars = ad.n_obs, ad.n_vars
    del ad; emergency_gc()
    print(f"   • Saved cleaned AnnData: {n_obs:,} cells × {n_vars:,} genes")
    return n_obs, n_vars

def compute_or_load_shards(n_items, num_shards, out_dir, seed=0):
    """Create/Load shards with metadata integrity check."""
    path = os.path.join(out_dir, "shard_indices.npz")
    def _save(idx_list):
        meta = dict(meta_n_items=n_items, meta_num_shards=num_shards, meta_seed=seed)
        np.savez_compressed(path, **{f"shard{i}": idx_list[i] for i in range(num_shards)}, **meta)
        print(f"   ✓ Saved shards → {path}")
        return idx_list
    if os.path.exists(path):
        try:
            data = np.load(path, allow_pickle=True)
            mi = int(data["meta_n_items"]); ms = int(data["meta_num_shards"]); md = int(data["meta_seed"])
            shards = [data[f"shard{i}"].astype(int) for i in range(ms)]
            ok = (mi == n_items) and (ms == num_shards) and (md == seed) and (sum(len(s) for s in shards) == n_items)
            if ok:
                print(f"   ✓ Loaded existing shards from {path}")
                return shards
            else:
                print("   • Shard metadata mismatch; regenerating…")
        except Exception as e:
            print(f"   • Could not read shards ({e}); regenerating…")
    rng = np.random.default_rng(seed)
    idx = np.arange(n_items); rng.shuffle(idx)
    shards = [chunk.astype(int) for chunk in np.array_split(idx, num_shards)]
    return _save(shards)

def load_model_from_ckpt(ckpt_path):
    """Load CellFM weights, infer vocab rows if possible, and force fp32 on CPU."""
    from model import CellFM
    from config import Config_80M
    cfg = Config_80M()
    params = load_checkpoint(os.path.abspath(ckpt_path))
    rows = None
    for name, tensor in params.items():
        n = name.lower()
        if "gene_emb" in n or ("embedding_table" in n and "gene" in n):
            try:
                rows = int(tensor.shape[0]); break
            except Exception:
                pass
    if rows is None:
        rows = 27856  # default for 80M ckpt; 8 specials → n_genes = rows - 8
        print(f"   • Could not infer rows from ckpt; assuming {rows} (80M)")
    n_genes = max(1, rows - 8)
    model = CellFM(n_genes=n_genes, cfg=cfg)
    try:
        load_param_into_net(model, params, strict_load=True)
        print(f"   • Matched checkpoint: ckpt rows={rows}, model rows={rows} using n_genes={n_genes}")
    except RuntimeError as e:
        print(f"   • Strict load failed ({e}); retrying non-strict.")
        load_param_into_net(model, params, strict_load=False)
    del params; emergency_gc()
    model.set_train(False)
    model = model.to_float(mstype.float32)  # CPU-safe dtype
    return model

def _tqdm_iter(ms_dataset, desc="Embedding", total=None):
    if total is None:
        try:
            total = ms_dataset.get_dataset_size()
        except Exception:
            pass
    return tqdm(ms_dataset.create_dict_iterator(output_numpy=False, num_epochs=1),
                total=total, desc=desc)

def _range_has_target(start, end, target_set):
    if not target_set: return False
    for gi in range(start, end):
        if gi in target_set:
            return True
    return False

def embed_dataset(ms_dataset, model, obs_names, target_index_set,
                  out_h5, out_csv, embed_dim, micro_bsz=MICRO_BSZ):
    """Forward pass to extract CLS embeddings (embed_dim) for the given shard."""
    if os.path.exists(out_h5):
        os.remove(out_h5)
    saved_names, row = [], 0
    total_rows = len(target_index_set)
    with h5py.File(out_h5, "w") as f:
        dset = f.create_dataset(
            "cls", shape=(total_rows, embed_dim), dtype="float16",
            chunks=(min(512, max(1, total_rows)), embed_dim),
            compression="gzip", compression_opts=1
        )
        cursor = 0
        processed_minibatches = 0
        skipped_minibatches   = 0
        it = _tqdm_iter(ms_dataset, desc="Embedding (skips enabled)")
        for batch in it:
            N = next(iter(batch.values())).shape[0]
            if not _range_has_target(cursor, cursor + N, target_index_set):
                skipped_minibatches += 1
                cursor += N
                continue
            for s in range(0, N, micro_bsz):
                e = min(s + micro_bsz, N)
                if not _range_has_target(cursor + s, cursor + e, target_index_set):
                    skipped_minibatches += 1
                    continue
                mb = {k: v[s:e] for k, v in batch.items()}
                out = model(mb["raw_nzdata"], mb["masked_nzdata"],
                            mb["nonz_gene"], mb["mask_gene"], mb["zero_idx"])
                # Strict CLS extraction
                cls = None
                if isinstance(out, ms.Tensor):
                    if out.ndim == 3 and out.shape[-1] == embed_dim:
                        cls = out[:, 0, :]
                    elif out.ndim == 2 and out.shape[-1] == embed_dim:
                        cls = out
                    else:
                        raise RuntimeError(f"Unexpected model output shape {tuple(out.shape)}")
                elif isinstance(out, (tuple, list)):
                    for t in out:
                        if isinstance(t, ms.Tensor) and (t.ndim in (2,3)) and t.shape[-1] == embed_dim:
                            cls = (t[:, 0, :] if t.ndim == 3 else t)
                            break
                    if cls is None:
                        raise RuntimeError("Could not find a (B,embed_dim) or (B,L,embed_dim) tensor in model output")
                else:
                    raise RuntimeError(f"Model returned unsupported type: {type(out)}")
                cls_np = np.nan_to_num(cls.asnumpy()).astype(np.float16)
                for i in range(e - s):
                    global_idx = cursor + s + i
                    if global_idx in target_index_set:
                        dset[row] = cls_np[i]
                        saved_names.append(
                            str(obs_names[global_idx]) if global_idx < len(obs_names) else f"cell_{global_idx}"
                        )
                        row += 1
                processed_minibatches += 1
            cursor += N
            emergency_gc()
        it.close()
        print(f"   • Forward passes run: {processed_minibatches} | micro-batches skipped: {skipped_minibatches}")
    if row != total_rows:
        raise RuntimeError(f"Saved {row} / expected {total_rows} embeddings; index mapping is inconsistent.")
    pd.DataFrame({"obs_names": saved_names}).to_csv(out_csv, index=False)
    return row

# ============================== MAIN =========================================
def main():
    print("🚀 CellFM — ZERO-SHOT (no train/test), shard-only compute")

    # 0) Sanitize the raw dataset → cleaned path (this is what SCrna will read)
    n_obs, n_vars = sanitize_and_save_h5ad(RAW_DATASETPATH, CLEAN_DATASETPATH)

    # 1) For quick metadata/overlap reporting, read cleaned in backed mode
    adata = sc.read_h5ad(CLEAN_DATASETPATH, backed="r")
    print(f"   • AnnData (cleaned, backed): {adata.n_obs:,} cells × {adata.n_vars:,} genes")
    obs_names = adata.obs_names

    # 2) Non-masked Prepare (zero-shot)
    from data_process import Prepare, SCrna, build_dataset
    prep = Prepare(EMBED_SEQ_CAP, pad=0, mask_ratio=0.0, random=False)

    # 3) SCrna expects (path, filename) — point it at the CLEANED file
    ds_dir, ds_file = os.path.dirname(CLEAN_DATASETPATH), os.path.basename(CLEAN_DATASETPATH)
    scrna = SCrna(path=ds_dir, data=ds_file, filt_len=(200, EMBED_SEQ_CAP), prep=True)

    # 4) Optional: overlap with gene_info.csv
    try:
        gi = pd.read_csv(os.path.join(REPO_ROOT, "gene_info.csv"))
        cand_cols = [c for c in gi.columns if str(c).lower() in ("symbol", "gene", "gene_name")]
        if not cand_cols: cand_cols = [gi.columns[0]]
        vocab = set(gi[cand_cols[0]].astype(str))
        ad_syms = set(map(str, getattr(adata, "var_names")))
        overlap = len(vocab & ad_syms)
        print(f"   • Overlap with gene_info.csv using var_names: {overlap}/{getattr(adata, 'n_vars', len(ad_syms))}")
    except Exception as e:
        print(f"   • Overlap check skipped ({e})")

    # 5) MindSpore Dataset
    dataset = build_dataset(scrna, prep, batch=BATCH_SIZE, drop=False, pad_zero=True, shuffle=False)

    # 6) One-time dataset columns sanity-check
    sample = next(dataset.create_dict_iterator(output_numpy=False, num_epochs=1))
    need = {"raw_nzdata","masked_nzdata","nonz_gene","mask_gene","zero_idx"}
    missing = need - set(sample.keys())
    if missing:
        raise KeyError(f"Dataset is missing columns: {sorted(missing)}; found {sorted(sample.keys())}")
    for k, v in sample.items():
        try:
            print(f"   • sample[{k}] shape: {tuple(v.shape)}")
        except Exception:
            print(f"   • sample[{k}] type: {type(v)}")

    # 7) Load pretrained CellFM (no training) and force fp32
    print("   • Loading pretrained CellFM weights...")
    model = load_model_from_ckpt(CKPT_PATH)

    # 8) Smoke test to detect embed_dim
    mb = {k: v[:min(2, next(iter(sample.values())).shape[0])] for k, v in sample.items()}
    out = model(mb["raw_nzdata"], mb["masked_nzdata"], mb["nonz_gene"], mb["mask_gene"], mb["zero_idx"])
    if isinstance(out, ms.Tensor):
        embed_dim = out.shape[-1] if out.ndim in (2,3) else None
    elif isinstance(out, (tuple, list)):
        embed_dim = None
        for t in out:
            if isinstance(t, ms.Tensor) and (t.ndim in (2,3)):
                embed_dim = t.shape[-1]; break
    else:
        embed_dim = None
    if not isinstance(embed_dim, (int, np.integer)) or embed_dim <= 0:
        raise RuntimeError(f"Could not detect embed dim from model output; got type={type(out)}")
    print(f"   • Detected CLS embed dim: {embed_dim}")

    # 9) Shard-only execution
    try:
        scrna_len = len(scrna)
        print("   • SCrna length:", scrna_len)
    except Exception:
        scrna_len = dataset.get_dataset_size()
        print("   • Dataset size (approx):", scrna_len)

    if not (0 <= BATCH_ID < NUM_SHARDS):
        raise ValueError(f"BATCH_ID {BATCH_ID} out of range 0..{NUM_SHARDS-1}")

    shards = compute_or_load_shards(scrna_len, NUM_SHARDS, SHARD_DIR, seed=SEED)
    target = set(map(int, shards[BATCH_ID]))
    print(f"   → SHARD {BATCH_ID}/{NUM_SHARDS-1} | target rows={len(target):,}")

    # 10) Outputs for this shard
    part_h5  = os.path.join(PARTS_DIR, f"cls_embeddings.part{BATCH_ID}.h5")
    part_csv = os.path.join(PARTS_DIR, f"cls_embeddings.part{BATCH_ID}.csv")

    # 11) Forward pass (ZERO-SHOT)
    n_rows = embed_dataset(dataset, model, obs_names, target, part_h5, part_csv,
                           embed_dim=embed_dim, micro_bsz=MICRO_BSZ)
    print(f"✅ Shard {BATCH_ID} complete: {n_rows} embeddings saved → {part_h5}")

    if not DO_MERGE:
        return

    # 12) Merge all parts (run once after all shards generated)
    print("\n>>> MERGE mode: combining all shards...")
    name_to_vec = {}
    for i in range(NUM_SHARDS):
        p = os.path.join(PARTS_DIR, f"cls_embeddings.part{i}.h5")
        c = os.path.join(PARTS_DIR, f"cls_embeddings.part{i}.csv")
        if not (os.path.exists(p) and os.path.exists(c)):
            print(f"   • Missing shard {i} (skip)"); continue
        with h5py.File(p, "r") as f:
            arr = f["cls"][:].astype(np.float32)
        names = pd.read_csv(c)["obs_names"].astype(str).to_numpy()
        for vec, name in zip(arr, names):
            name_to_vec[name] = vec
    if not name_to_vec:
        raise RuntimeError("No embeddings found to merge!")
    ordered = sorted(name_to_vec.keys())
    E = np.vstack([name_to_vec[n] for n in ordered])

    merged_h5 = os.path.join(OUTDIR, "cls_embeddings.h5")
    with h5py.File(merged_h5, "w") as f:
        f.create_dataset("cls", data=E, dtype="float32",
                         chunks=(min(4096, E.shape[0]), E.shape[1]),
                         compression="gzip", compression_opts=1)
    pd.DataFrame({"obs_names": ordered}).to_csv(os.path.join(OUTDIR, "merged_obs_names.csv"), index=False)
    print(f"✅ Merged {len(ordered):,} embeddings → {merged_h5}")

    # 13) Optional UMAP
    try:
        ad_full = sc.read_h5ad(CLEAN_DATASETPATH)  # full load for viz only
        ad = ad_full[ordered].copy()
        ad.obsm["X_cellfm"] = E
        ad.obsm["X_pca"] = StandardScaler().fit_transform(E)
        sc.pp.neighbors(ad, use_rep="X_pca", n_neighbors=15)
        sc.tl.umap(ad)
        sc.pl.umap(ad, frameon=False, show=False)
        plt.savefig(os.path.join(FIGDIR, "umap_cellfm.png"), dpi=300)
        plt.close()
        print(f"🖼️ Saved UMAP to: {FIGDIR}/umap_cellfm.png")
    except Exception as e:
        print(f"⚠️ UMAP visualization failed: {e}")

if __name__ == "__main__":
    main()