# -*- coding: utf-8 -*-
"""knowledge-graph-CM4AI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vyEMp9V2RkjZ75p2yKBEyQg6zvmz9VqJ
"""

!pip install anndata scanpy mygene networkx goatools requests

from google.colab import drive
drive.mount('/content/drive')

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GEARS-ready GO graph builder & saver — Google Drive version
- DOES NOT modify your AnnData (assumes it's already GEARS-ready)
- Builds your cardiac knowledge graph exactly as in your snippet (GO + optional SIGNOR + coexpr fallback)
- Saves EVERYTHING directly into your Google Drive folders:
    <DRIVE_PROJECT_DIR>/
      ├── graphs/
      │    ├── cardiac_kg.graphml
      │    ├── cardiac_kg.pt           (if torch_geometric available)
      │    └── candidate_pert_list.tsv
      └── data/<DATASET_NAME>/
           └── go_essential_<DATASET_NAME>.csv   (edge list format GEARS expects)
"""

# ======================= USER PATHS (EDIT THESE) ==============================
ADATA_H5AD        = "/content/drive/MyDrive/iPSC_CM/datasets/CM4AI/KOLF2.1J_undifferentiated_untreated_chromatin.h5ad"
DRIVE_PROJECT_DIR = "/content/drive/MyDrive/iPSC_CM/CM4AI_KG"
DATASET_NAME      = "kolf2"   # used for GEARS CSV name: go_essential_<DATASET_NAME>.csv
# =============================================================================

import os, io, json, gzip, warnings, logging
from typing import List, Tuple, Dict
import numpy as np
import pandas as pd
import anndata as ad
import scanpy as sc
import networkx as nx

# -------- Mount Google Drive (Colab) --------
try:
    from google.colab import drive
    drive.mount("/content/drive", force_remount=False)
    print("✓ Google Drive mounted")
except Exception as e:
    print(f"ℹ️ Skipping Drive mount: {e}")

# Quiet the mygene/biothings chatter
logging.getLogger("biothings.client").setLevel(logging.ERROR)

# Optional: PyG for .pt export of the graph (silently skip if unavailable)
import torch
try:
    from torch_geometric.data import Data as PyGData
except Exception:
    PyGData = None
    warnings.warn("torch_geometric not found; will skip .pt export")

# --------------------------
# 0) Drive folders
# --------------------------
GRAPHS_DIR = os.path.join(DRIVE_PROJECT_DIR, "graphs")
DATA_DIR   = os.path.join(DRIVE_PROJECT_DIR, "data", DATASET_NAME)
os.makedirs(GRAPHS_DIR, exist_ok=True)
os.makedirs(DATA_DIR,   exist_ok=True)

# --------------------------
# 1) Load your AnnData (NO formatting here)
# --------------------------
adata = ad.read_h5ad(ADATA_H5AD)
print(f"AnnData loaded: {adata.n_obs:,} cells × {adata.n_vars:,} features")

# Gene vocabulary (assumes 'gene_name' already present/cleaned; otherwise falls back to var_names IN-MEMORY ONLY)
if 'gene_name' in adata.var.columns and adata.var['gene_name'].notna().any():
    gene_list = adata.var['gene_name'].astype(str).tolist()
else:
    gene_list = adata.var_names.astype(str).tolist()
genes = set(map(str, gene_list))

# --------------------------
# 2) Build your cardiac knowledge graph (GO + optional SIGNOR + coexpr fallback)
# --------------------------
G = nx.MultiDiGraph()
G.add_nodes_from(genes)

# 2A. SIGNOR: directed causal interactions (optional; internet required)
def fetch_signor_edges(gene_list: List[str]) -> List[Tuple[str, str, Dict]]:
    import requests
    edges = []
    base = 'https://signor.uniroma2.it/api/get_interactions.php'
    for g in gene_list:
        try:
            r = requests.get(base, params={'organism': '9606', 'format': 'tsv', 'source': g}, timeout=30)
            if r.ok and r.text.strip():
                df = pd.read_csv(io.StringIO(r.text), sep='\t')
                if not {'ENTITYA','ENTITYB'}.issubset(df.columns):
                    continue
                for _, row in df.iterrows():
                    s = str(row.get('ENTITYA', ''))
                    t = str(row.get('ENTITYB', ''))
                    if s in genes and t in genes:
                        G.add_edge(s, t, source_db='SIGNOR',
                                   mechanism=row.get('MECHANISM', None),
                                   effect=row.get('EFFECT', None),
                                   pmid=row.get('PMID', None),
                                   weight=1.0)
        except Exception as e:
            print(f"[WARN] SIGNOR fetch failed for {g}: {e}")
    return list(G.edges(data=True))

# Cap for speed; increase/remove cap as needed
_ = fetch_signor_edges(sorted(list(genes))[:400])

# 2B. Robust GO edges + guaranteed fallback so KG is never empty
def build_go_edges_v3(genes: set, max_pairs_per_go: int = 5000,
                      aspects=('P',), evidence_keep=None, verbose=True) -> int:
    """
    Build undirected co-annotation edges from NCBI gene2go.gz using pandas
    with a robust header and Aspect normalization.
    aspects: tuple of desired aspects ('P','F','C'); default 'P' = Biological Process
    evidence_keep: optional set of GO evidence codes to keep (e.g., {'EXP','IDA','IMP','IPI','IGI','TAS'})
    """
    import requests, pandas as pd, os
    gene2go_path = 'gene2go.gz'
    if not os.path.exists(gene2go_path):
        url = 'https://ftp.ncbi.nlm.nih.gov/gene/DATA/gene2go.gz'
        with requests.get(url, stream=True, timeout=120) as r:
            r.raise_for_status()
            with open(gene2go_path, 'wb') as f:
                for chunk in r.iter_content(chunk_size=1 << 20):
                    if chunk: f.write(chunk)

    # Assign the exact header; treat '#' as comments so commented header is skipped safely.
    colnames = ["tax_id","GeneID","GO_ID","Evidence","Qualifier","GO_term","PubMed","Category"]
    df = pd.read_csv(gene2go_path, sep='\t', compression='infer',
                     comment='#', header=None, names=colnames, dtype=str, low_memory=False)

    # Human only
    df = df[df['tax_id'] == '9606']
    if df.empty:
        if verbose: print("[WARN] gene2go has no human rows; skipping GO edges.")
        return 0

    # Normalize Aspect/Category to P/F/C
    cat = df['Category'].str.strip().str.upper()
    cat = cat.replace({'PROCESS':'P','FUNCTION':'F','COMPONENT':'C'})
    df['Aspect'] = cat
    df = df[df['Aspect'].isin([a.upper() for a in aspects])]

    # Optional: keep only higher-confidence evidence codes
    if evidence_keep is not None:
        df = df[df['Evidence'].isin(evidence_keep)]

    # Map Entrez → HGNC symbols present in your vocabulary
    from mygene import MyGeneInfo
    mg = MyGeneInfo()
    entrez_ids = list(df['GeneID'].dropna().unique())
    mapping = {}
    for i in range(0, len(entrez_ids), 1000):
        chunk = entrez_ids[i:i+1000]
        q = mg.querymany(chunk, scopes='entrezgene', fields='symbol', species='human',
                         as_dataframe=True, returnall=False, verbose=False)
        if isinstance(q, pd.DataFrame) and 'symbol' in q.columns:
            mapping.update(q['symbol'].dropna().astype(str).to_dict())

    df['symbol'] = df['GeneID'].map(mapping)
    df = df[df['symbol'].isin(genes)]

    # Build symmetric co-annotation edges capped per GO_ID
    added = 0
    for go_id, group in df.groupby('GO_ID'):
        syms = group['symbol'].dropna().unique().tolist()
        if len(syms) < 2: continue
        count = 0
        for i in range(len(syms)):
            for j in range(i+1, len(syms)):
                s, t = syms[i], syms[j]
                if s in genes and t in genes:
                    payload = {'source_db':'GO', 'relation':'co_annotated',
                               'go_term':go_id, 'aspect':group['Aspect'].iloc[0], 'weight':0.7}
                    G.add_edge(s, t, **payload)
                    G.add_edge(t, s, **payload)
                    added += 2
                    count += 2
                    if count >= max_pairs_per_go: break
            if count >= max_pairs_per_go: break
    if verbose: print(f"   • GO edges added: {added}")
    return added

# --- call it (use stronger evidence if you like) ---
EVIDENCE = None  # e.g., {'EXP','IDA','IMP','IPI','IGI','TAS'} for stricter edges
go_edges = build_go_edges_v3(genes, aspects=('P',), evidence_keep=EVIDENCE)

# If SIGNOR produced few edges and GO fetch failed or is sparse, add a local coexpression fallback.
def add_coexpression_edges_from_adata(adata, genes_set, topk=3, tau=0.3, verbose=True):
    """
    Lightweight fallback: compute gene–gene Spearman r on a small HVG set and add edges where r>=tau.
    Uses topk for sparsity. Keeps direction both ways with weight=r.
    """
    import numpy as np, scanpy as sc
    A = adata.copy()
    sc.pp.highly_variable_genes(A, flavor='seurat_v3', n_top_genes=min(2000, A.n_vars))
    A = A[:, A.var['highly_variable']].copy()
    # align to your vocabulary
    keep = [g for g in A.var_names if g in genes_set]
    if len(keep) < 2:
        if verbose: print("   • Coexpression fallback skipped (insufficient overlap).")
        return 0
    A = A[:, keep]
    X = A.X.toarray() if hasattr(A.X,'toarray') else np.asarray(A.X)
    from scipy.stats import spearmanr
    rho = spearmanr(X, axis=0).correlation
    np.fill_diagonal(rho, 0.0)
    # Add top-k per gene over threshold
    added = 0
    gene_order = list(A.var_names)
    for i, gi in enumerate(gene_order):
        idx = np.argwhere(rho[i] >= tau).flatten()
        if idx.size == 0: continue
        top = idx[np.argsort(rho[i, idx])[-topk:]]
        for j in top:
            gj = gene_order[j]
            if gi in genes_set and gj in genes_set:
                w = float(rho[i, j])
                G.add_edge(gi, gj, source_db='COEXPR', relation='rho', weight=w)
                G.add_edge(gj, gi, source_db='COEXPR', relation='rho', weight=w)
                added += 2
    if verbose: print(f"   • Coexpression fallback edges added: {added} (tau={tau}, topk={topk})")
    return added

if G.number_of_edges() == 0 or go_edges == 0:
    _ = add_coexpression_edges_from_adata(adata, genes, topk=3, tau=0.3)

print(f"Cardiac KG: {G.number_of_nodes()} nodes, {G.number_of_edges()} directed edges")

# --------------------------
# 3) SAVE — directly to Google Drive
# --------------------------
# 3A. GraphML
graphml_path = os.path.join(GRAPHS_DIR, "cardiac_kg.graphml")
nx.write_graphml(G, graphml_path)

# 3B. PyG .pt (optional)
if PyGData is not None:
    ordered_genes = list(map(str, gene_list))
    gene2idx = {g: i for i, g in enumerate(ordered_genes) if g in G}
    src, dst, w = [], [], []
    for u, v, attr in G.edges(data=True):
        if u in gene2idx and v in gene2idx:
            src.append(gene2idx[u]); dst.append(gene2idx[v])
            w.append(float(attr.get('weight', 1.0)))
    if len(src) == 0:
        edge_index = torch.empty((2,0), dtype=torch.long)
        edge_weight = torch.empty((0,), dtype=torch.float32)
    else:
        edge_index = torch.tensor([src, dst], dtype=torch.long)
        edge_weight = torch.tensor(w, dtype=torch.float32)
    x = torch.zeros((len(ordered_genes), 1), dtype=torch.float32)
    pyg = PyGData(x=x, edge_index=edge_index, edge_attr=edge_weight.unsqueeze(1))
    pt_path = os.path.join(GRAPHS_DIR, "cardiac_kg.pt")
    torch.save({'gene_index': ordered_genes, 'data': pyg}, pt_path)
else:
    pt_path = None
    warnings.warn("torch_geometric not found; skipped .pt export")

# 3C. Candidate perturbation list
pert_path = os.path.join(GRAPHS_DIR, "candidate_pert_list.tsv")
pd.Series(sorted(list(genes))).to_csv(pert_path, index=False, header=False)

# 3D. GEARS-friendly GO CSV (source,target,importance)
rows = []
for u, v, attr in G.edges(data=True):
    if str(attr.get("source_db", "")).upper() == "GO":
        rows.append((str(u), str(v), float(attr.get("weight", 1.0))))
df_go = pd.DataFrame(rows, columns=["source", "target", "importance"])
# Deduplicate parallel edges, keep strongest
df_go = (df_go.sort_values("importance", ascending=False)
              .drop_duplicates(subset=["source", "target"], keep="first"))
go_csv_path = os.path.join(DATA_DIR, f"go_essential_{DATASET_NAME}.csv")
df_go.to_csv(go_csv_path, index=False)

# --------------------------
# 4) Done — print absolute Drive locations
# --------------------------
print("\nWrote to Google Drive:")
print(f"  - GraphML:   {graphml_path}")
if pt_path is not None:
    print(f"  - PyG .pt:   {pt_path}")
print(f"  - Pert list: {pert_path}")
print(f"  - GEARS GO:  {go_csv_path}   (place GEARS dataset here: {os.path.dirname(go_csv_path)})")

import pandas as pd
pert = set(pd.read_csv("results/graphs/candidate_pert_list.tsv", header=None)[0].astype(str))
P = nx.read_graphml("results/graphs/cardiac_kg_pruned.graphml")
U = P.to_undirected()
giant_nodes = set(max(nx.connected_components(U), key=len)) if U.number_of_nodes()>0 else set()
in_vocab = pert & set(P.nodes())
in_giant = in_vocab & giant_nodes
print(f"Perturbation genes in graph: {len(in_vocab)}/{len(pert)}")
print(f"Perturbation genes in giant component: {len(in_giant)}/{len(in_vocab)} ({len(in_giant)/max(1,len(in_vocab)):.1%})")

deg = dict(P.degree())  # undirected degree for hub check
vals = np.array(list(deg.values()))
for q in (50,90,95,99):
    print(f"degree p{q}: {np.percentile(vals,q):.0f}")