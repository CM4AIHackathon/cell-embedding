# -*- coding: utf-8 -*-
"""gears-cm4ai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NG8W302yRH99uyC6Xo8hhHjObn5LdNMW
"""

!git clone https://github.com/snap-stanford/GEARS/

# Commented out IPython magic to ensure Python compatibility.
# %cd GEARS/
!pip install -r requirements.txt

from google.colab import drive
drive.mount('/content/drive')

#!/usr/bin/env python3
# Make var['gene_name'] from var_names, save to Drive, then load GEARS with custom GO

import anndata

adata = anndata.read_h5ad("/content/drive/MyDrive/ready_anndata_KOLF2.1J.h5ad")
adata

# ---- USER PATHS ----
IN_ADATA_H5AD   = "/content/drive/MyDrive/ready_anndata_KOLF2.1J_fixed.h5ad"
OUT_ADATA_H5AD  = "/content/drive/MyDrive/ready_anndata_KOLF2.1J_with_gene_name.h5ad"
DATA_ROOT       = "/content/drive/MyDrive/iPSC_CM/CM4AI_KG/data"   # contains kolf2/
DATASET_NAME    = "kolf2"
GO_CSV          = f"{DATA_ROOT}/{DATASET_NAME}/go_essential_{DATASET_NAME}.csv"
# --------------------

import os, pandas as pd, numpy as np, scanpy as sc

# 1) Load AnnData (no reformatting besides adding var['gene_name'] if missing)
adata = sc.read_h5ad(IN_ADATA_H5AD)
print(f"Loaded: {adata.n_obs:,} cells × {adata.n_vars:,} vars")

if 'gene_name' not in adata.var.columns:
    print("→ Adding var['gene_name'] from var_names (no changes to var_names).")
    adata.var['gene_name'] = adata.var_names.astype(str)
else:
    print("✓ var['gene_name'] already present.")

# Minimal sanity
if adata.var['gene_name'].isna().any() or (adata.var['gene_name'].astype(str) == '').any():
    raise ValueError("Empty/NaN values found in var['gene_name']; please clean upstream.")

# Optional: warn on duplicates (GEARS prefers 1:1 gene mapping)
dups = adata.var['gene_name'].astype(str).duplicated(keep='first')
if dups.any():
    ndup = int(dups.sum())
    print(f"[WARN] {ndup} duplicate gene_name entries. GEARS may still load, "
          "but de-duplicating upstream is recommended.")

# 2) Save the augmented file to Drive
adata.write_h5ad(OUT_ADATA_H5AD, compression="gzip")
print(f"✓ Saved → {OUT_ADATA_H5AD}")

# 3) Coverage check vs your GO CSV (optional but helpful)
assert os.path.exists(GO_CSV), f"Missing GO CSV at {GO_CSV}"
df_go = pd.read_csv(GO_CSV)
assert {'source','target','importance'}.issubset(df_go.columns), "GO CSV must have columns: source,target,importance"
go_genes = set(df_go['source'].astype(str)) | set(df_go['target'].astype(str))
ad_genes = set(adata.var['gene_name'].astype(str))
overlap  = len(go_genes & ad_genes)
print(f"GO edges: {len(df_go):,} | GO genes: {len(go_genes):,} | "
      f"overlap with gene_name: {overlap:,}/{len(ad_genes):,} ({overlap/max(1,len(ad_genes)):.1%})")

# 4) Load GEARS using the file that now has var['gene_name'] and your custom GO
try:
    import gears  # noqa: F401
except Exception:
    # quiet install if missing
    import sys, subprocess
    subprocess.run([sys.executable, "-m", "pip", "install", "-q", "cell-gears"], check=True)

from gears import PertData, GEARS

#!/usr/bin/env python3
# Reload processed GEARS dataset, align & attach custom GO graph, count represented perturbations

# ---------- USER PATHS ----------
BASE          = "/content/drive/MyDrive/iPSC_CM/CM4AI_KG"
DATASET_NAME  = "kolf2"
GO_CSV        = f"{BASE}/data/{DATASET_NAME}/go_essential_{DATASET_NAME}.csv"
BATCH_SIZE    = 32
TEST_BSZ      = 128
SEED          = 1
USE_NO_TEST   = True
TRAIN_RATIO   = 0.75
# ---------------------------------

import os, sys, glob, json, pandas as pd
from pathlib import Path

# --- Make paths stable; GEARS expects a working dir with a literal "./data"
BASE = os.path.abspath(BASE)
Path(BASE).mkdir(parents=True, exist_ok=True)
os.chdir(BASE)
DATA_DIR = Path("./data"); DATA_DIR.mkdir(parents=True, exist_ok=True)
kolf2_path = DATA_DIR / DATASET_NAME

print("Working directory:", Path.cwd())
print("Dataset folder  :", kolf2_path)

# --- GEARS import (install only if missing) ---
try:
    from gears import PertData, GEARS
except Exception:
    import subprocess
    subprocess.run([sys.executable, "-m", "pip", "install", "-q", "cell-gears"], check=True)
    from gears import PertData, GEARS

# --- Helper: device auto ---
def device_auto():
    try:
        import torch
        return "cuda" if torch.cuda.is_available() else "cpu"
    except Exception:
        return "cpu"

# --- 1) Load processed dataset (no rebuilding pyg) ---
if not kolf2_path.exists():
    raise FileNotFoundError(f"Processed dataset not found at {kolf2_path} (expected ./data/{DATASET_NAME}).")

print("Found local copy...")
pert = PertData("./data", default_pert_graph=False)  # force using local/custom GO
pert.load(data_path=str(kolf2_path))                 # loads cached ./data/kolf2

# Build the dataset vocabulary used for graph alignment
if getattr(pert, "node_map", None):
    vocab = set(map(str, pert.node_map.keys()))
else:
    # fallback to AnnData
    if "gene_name" in pert.adata.var.columns:
        vocab = set(pert.adata.var["gene_name"].astype(str))
    else:
        vocab = set(pert.adata.var_names.astype(str))
print(f"#genes in dataset vocabulary: {len(vocab):,}")

# --- 2) Load a non-empty GO CSV (with fallback if your primary file is empty) ---
def load_go_csv_with_fallback(primary_path: str) -> pd.DataFrame:
    def _read(p):
        df = pd.read_csv(p)
        # tolerate either weight/importance; minimal header validation later
        return df
    cand = [primary_path]
    # try any other go*.csv in the same folder (pick the largest non-empty)
    folder = os.path.dirname(primary_path)
    for alt in glob.glob(os.path.join(folder, "go*.csv")):
        if alt != primary_path:
            cand.append(alt)
    # sort by file size desc so we prefer real, non-empty files
    cand = sorted(cand, key=lambda p: os.path.getsize(p) if os.path.exists(p) else -1, reverse=True)
    for p in cand:
        if os.path.exists(p) and os.path.getsize(p) > 0:
            df = _read(p)
            if df.shape[0] > 0:
                print(f"Using GO CSV: {p}  (rows={len(df):,})")
                return df
    raise ValueError(f"All candidate GO CSVs are empty/missing. Check: {primary_path}")

assert os.path.exists(os.path.dirname(GO_CSV)), f"Folder missing: {os.path.dirname(GO_CSV)}"
df_go = load_go_csv_with_fallback(GO_CSV)

needed = {"source","target"}
if not needed.issubset(df_go.columns):
    raise ValueError(f"GO CSV must have columns at least: {needed}. Found: {df_go.columns.tolist()}")

# --- 3) Harmonize gene symbols + filter to dataset vocab (prevents KeyErrors) ---
for col in ("source","target"):
    df_go[col] = df_go[col].astype(str).str.strip()

ALIASES = {
    # fixes for your cohort
    "ARTD1": "PARP1", "ARDT1": "PARP1",
    "BRG1": "SMARCA4",
    "MSK1": "RPS6KA1", "MSK2": "RPS6KA4",
    "BRE1A": "RNF20",
    "KAT3A": "CREBBP", "KAT3B": "EP300",
}
df_go["source"] = df_go["source"].map(lambda g: ALIASES.get(g, g))
df_go["target"] = df_go["target"].map(lambda g: ALIASES.get(g, g))

m_before = len(df_go)
mask = df_go["source"].isin(vocab) & df_go["target"].isin(vocab)
df_go = df_go.loc[mask].copy()

# optional: drop known stragglers that cause issues in your env
banlist = {"FTX"}  # lncRNA often outside vocab; drop if present
df_go = df_go[~df_go["source"].isin(banlist) & ~df_go["target"].isin(banlist)].copy()

if "importance" not in df_go.columns and "weight" in df_go.columns:
    df_go = df_go.rename(columns={"weight": "importance"})

kept_genes = sorted(set(df_go["source"]) | set(df_go["target"]))
print(f"GO edges before/after filter: {m_before:,} → {len(df_go):,} (unique genes kept: {len(kept_genes):,})")

if len(df_go) == 0:
    raise ValueError("Filtered GO graph is empty after alignment. Likely wrong symbols/vocab. "
                     "Inspect a few entries from your GO CSV and your pert.adata.var['gene_name'].")

# Overwrite the EXACT path GEARS reads
df_go.to_csv(GO_CSV, index=False)
print(f"✓ Wrote filtered GO to: {GO_CSV}")

# --- 4) Ensure perturbed gene list is subset of vocab (avoid 'FTX' KeyError) ---
if hasattr(pert, "pert_genes"):
    bad = [g for g in pert.pert_genes if g not in vocab]
    if bad:
        print(f"[WARN] Removing {len(bad)} genes not in vocab from pert.pert_genes (e.g., {bad[:5]})")
        pert.pert_genes = [g for g in pert.pert_genes if g in vocab]

# (Optional) You can also restrict with a custom gene_set CSV (API supported in issues)
# from GEARS issues: PertData('./data', gene_set_path='/path/to/gene_list.csv')  # CSV of allowed genes
# This helps if your processed dataset includes extra symbols you want to prune. :contentReference[oaicite:1]{index=1}

# --- 5) Splits + dataloaders (must be done before model init) ---
if USE_NO_TEST:
    try:
        pert.prepare_split(split="no_test", seed=SEED, ratio=TRAIN_RATIO)
    except TypeError:
        pert.prepare_split(split="no_test", seed=SEED)
else:
    pert.prepare_split(split="simulation", seed=SEED)

pert.get_dataloader(batch_size=BATCH_SIZE, test_batch_size=TEST_BSZ)
print("Dataloaders built ✓")

# --- 6) Initialize GEARS (reads aligned GO) ---
gears_model = GEARS(pert, device=device_auto())
gears_model.model_initialize(hidden_size=64)
print("Model initialized ✓")

print("\n=== GEARS READY (custom GO aligned) ===")
print("  dataset :", DATASET_NAME)
print("  GO file :", GO_CSV)
print("  #genes  :", pert.adata.var.shape[0], "| #cells:", pert.adata.n_obs)

# --- 7) Count represented vs missing perturbations wrt GO nodes ---
def parse_pert_tokens(cond: str):
    toks = [t.strip() for t in str(cond).split('+') if t.strip()]
    return {t for t in toks if t.lower() not in {"ctrl", "control"}}

from collections import defaultdict
if 'condition' not in pert.adata.obs.columns:
    raise KeyError("pert.adata.obs lacks 'condition'.")

conds = sorted(set(map(str, pert.adata.obs['condition'])))
non_ctrl = [c for c in conds if c.lower() not in {"ctrl","control"}]

go_nodes = set(kept_genes)  # nodes present in GO after filtering
represented, missing = [], []
missing_detail = defaultdict(list)

for c in non_ctrl:
    targets = parse_pert_tokens(c)
    miss = sorted([g for g in targets if g not in go_nodes])
    if miss:
        missing.append(c); missing_detail[c] = miss
    else:
        represented.append(c)

print("\n--- Perturbation coverage in GO graph ---")
print(f"Total unique perturbations (excl. ctrl): {len(non_ctrl):,}")
print(f"Represented in GO (all target genes present): {len(represented):,}")
print(f"Missing from GO (≥1 target gene absent):      {len(missing):,}")
if missing:
    print("Examples of missing:", missing[:10])
    # Save a full report for auditing
    miss_rows = [{"perturbation": k, "missing_genes": ";".join(v)} for k,v in missing_detail.items()]
    pd.DataFrame(miss_rows).to_csv(f"{BASE}/data/{DATASET_NAME}/missing_perturbations_vs_GO.csv", index=False)
    print(f"Saved: {BASE}/data/{DATASET_NAME}/missing_perturbations_vs_GO.csv")

print("\nSuccess ✅ — GEARS is using your GO graph and coverage is reported.")

#!/usr/bin/env python3
# Fixed gene mapping code to properly upload your cardiac knowledge graph to GEARS

import pandas as pd
import numpy as np
from pathlib import Path
import os

# ======================= USER PATHS (EDIT THESE) ==============================
BASE_PATH = "/content/drive/MyDrive/iPSC_CM/CM4AI_KG"
DATASET_NAME = "kolf2"
# =============================================================================

def install_mygene():
    """Install mygene for gene name conversion"""
    try:
        import mygene
        return mygene
    except ImportError:
        import subprocess
        import sys
        subprocess.check_call([sys.executable, "-m", "pip", "install", "mygene"])
        import mygene
        return mygene

def map_genes_with_mygene(gene_list, target_vocab_set, source_species="human"):
    """
    Use MyGene.info service to map gene names to target vocabulary
    Returns: (gene_mapping_dict, unmapped_genes_list)
    """
    mg = install_mygene()
    mg_client = mg.MyGeneInfo()

    print(f"🔍 Mapping {len(gene_list)} genes using MyGene.info...")

    # Query for gene information in batches (API rate limiting)
    gene_mapping = {}
    unmapped = []
    batch_size = 1000

    for i in range(0, len(gene_list), batch_size):
        batch = gene_list[i:i + batch_size]
        try:
            results = mg_client.querymany(
                batch,
                scopes=['symbol', 'alias', 'ensembl.gene', 'entrezgene'],
                fields='symbol,alias,ensembl.gene',
                species=source_species,
                returnall=True
            )

            for query, result in zip(batch, results['out']):
                mapped_gene = None

                # Priority 1: Check if official symbol is in target vocab
                if result.get('symbol') and result['symbol'] in target_vocab_set:
                    mapped_gene = result['symbol']

                # Priority 2: Check aliases
                elif result.get('alias'):
                    aliases = result['alias'] if isinstance(result['alias'], list) else [result['alias']]
                    for alias in aliases:
                        if str(alias) in target_vocab_set:
                            mapped_gene = str(alias)
                            break

                # Priority 3: Check if original query is in target vocab
                elif query in target_vocab_set:
                    mapped_gene = query

                if mapped_gene:
                    gene_mapping[query] = mapped_gene
                else:
                    unmapped.append(query)

        except Exception as e:
            print(f"   ⚠️ Batch mapping failed: {e}")
            # Add batch to unmapped on failure
            unmapped.extend(batch)

    successful_mappings = len(gene_mapping)
    print(f"   ✅ Successfully mapped {successful_mappings}/{len(gene_list)} genes")
    print(f"   ❌ Unmapped genes: {len(unmapped)}")

    return gene_mapping, unmapped

def create_proper_cardiac_go_alignment(cardiac_go_path, dataset_genes, use_mygene=True):
    """
    Properly align YOUR cardiac knowledge graph with GEARS dataset vocabulary
    """
    print("=" * 60)
    print("🧬 ALIGNING YOUR CARDIAC KNOWLEDGE GRAPH WITH GEARS")
    print("=" * 60)

    # Load your cardiac knowledge graph
    if not os.path.exists(cardiac_go_path):
        raise FileNotFoundError(f"❌ Cardiac KG not found at: {cardiac_go_path}")

    go_df = pd.read_csv(cardiac_go_path)
    print(f"📊 Loaded cardiac KG: {len(go_df):,} edges")

    # Validate columns
    required_cols = {'source', 'target'}
    if not required_cols.issubset(go_df.columns):
        raise ValueError(f"❌ Expected columns {required_cols}, found: {list(go_df.columns)}")

    # Clean gene names
    go_df['source'] = go_df['source'].astype(str).str.strip()
    go_df['target'] = go_df['target'].astype(str).str.strip()

    # Get all unique genes from your cardiac KG
    cardiac_kg_genes = list(set(go_df['source'].tolist() + go_df['target'].tolist()))
    print(f"🧬 Unique genes in cardiac KG: {len(cardiac_kg_genes):,}")

    # Convert dataset genes to set for fast lookup
    dataset_genes_set = set(map(str, dataset_genes))
    cardiac_kg_genes_set = set(cardiac_kg_genes)

    # Find genes that need mapping
    genes_already_mapped = cardiac_kg_genes_set & dataset_genes_set
    genes_needing_mapping = cardiac_kg_genes_set - dataset_genes_set

    print(f"✅ Genes already in GEARS vocab: {len(genes_already_mapped):,}")
    print(f"🔄 Genes needing mapping: {len(genes_needing_mapping):,}")

    if len(genes_needing_mapping) == 0:
        print("🎉 All cardiac KG genes already in GEARS vocabulary!")
        # Just filter to ensure clean data
        final_mask = (go_df['source'].isin(dataset_genes_set) &
                     go_df['target'].isin(dataset_genes_set))
        return go_df.loc[final_mask].copy()

    # Use MyGene.info for mapping if reasonable number of genes
    if use_mygene and len(genes_needing_mapping) < 5000:
        print("🔍 Using MyGene.info for gene mapping...")
        gene_mapping, still_unmapped = map_genes_with_mygene(
            list(genes_needing_mapping),
            dataset_genes_set
        )

        print(f"📋 Mapping results:")
        print(f"   ✅ Successfully mapped: {len(gene_mapping):,}")
        print(f"   ❌ Still unmapped: {len(still_unmapped):,}")

        if still_unmapped:
            print(f"   Examples of unmapped: {sorted(still_unmapped)[:10]}")

        # Apply mapping to your cardiac GO dataframe
        print("🔄 Applying mapping to cardiac knowledge graph...")
        go_df_mapped = go_df.copy()

        # Apply gene mapping
        go_df_mapped['source'] = go_df_mapped['source'].map(lambda x: gene_mapping.get(x, x))
        go_df_mapped['target'] = go_df_mapped['target'].map(lambda x: gene_mapping.get(x, x))

        # Filter to only keep edges where both genes are in GEARS dataset
        final_mask = (go_df_mapped['source'].isin(dataset_genes_set) &
                     go_df_mapped['target'].isin(dataset_genes_set))
        go_df_final = go_df_mapped.loc[final_mask].copy()

        print(f"📊 Final results:")
        print(f"   Original edges: {len(go_df):,}")
        print(f"   Mapped edges: {len(go_df_final):,}")
        print(f"   Retention rate: {len(go_df_final)/len(go_df):.1%}")

        return go_df_final

    else:
        print("⚠️ Too many genes for API mapping, using exact matching only")
        exact_mask = (go_df['source'].isin(dataset_genes_set) &
                     go_df['target'].isin(dataset_genes_set))
        go_df_exact = go_df.loc[exact_mask].copy()

        print(f"📊 Exact matching results:")
        print(f"   Original edges: {len(go_df):,}")
        print(f"   Exact match edges: {len(go_df_exact):,}")
        print(f"   Retention rate: {len(go_df_exact)/len(go_df):.1%}")

        return go_df_exact

def upload_cardiac_kg_to_gears(base_path=BASE_PATH, dataset_name=DATASET_NAME):
    """
    Main function to upload your cardiac knowledge graph to GEARS
    """
    print("🚀 UPLOADING YOUR CARDIAC KNOWLEDGE GRAPH TO GEARS")
    print("=" * 60)

    # Import GEARS
    try:
        from gears import PertData
    except ImportError:
        import subprocess
        import sys
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", "cell-gears"])
        from gears import PertData

    # Set up paths
    os.chdir(base_path)
    dataset_path = f"./data/{dataset_name}"
    cardiac_kg_path = f"./data/{dataset_name}/go_essential_{dataset_name}.csv"

    if not os.path.exists(cardiac_kg_path):
        # Try alternative locations
        alt_paths = [
            f"./graphs/cardiac_kg.csv",
            f"./data/{dataset_name}/go_essential_{dataset_name}.orig.csv"
        ]
        for alt_path in alt_paths:
            if os.path.exists(alt_path):
                cardiac_kg_path = alt_path
                break
        else:
            raise FileNotFoundError(f"❌ Could not find cardiac KG. Tried: {cardiac_kg_path}, {alt_paths}")

    print(f"📁 Using cardiac KG: {cardiac_kg_path}")

    # Load GEARS dataset to get vocabulary
    print("📊 Loading GEARS dataset to get vocabulary...")
    pert = PertData("./data", default_pert_graph=False)
    pert.load(data_path=dataset_path)

    # Get GEARS vocabulary (the authoritative source)
    if hasattr(pert, 'node_map') and pert.node_map:
        dataset_genes = list(map(str, pert.node_map.keys()))
        vocab_source = "node_map"
    else:
        if 'gene_name' in pert.adata.var.columns:
            dataset_genes = pert.adata.var['gene_name'].astype(str).tolist()
            vocab_source = "gene_name"
        else:
            dataset_genes = pert.adata.var_names.astype(str).tolist()
            vocab_source = "var_names"

    print(f"🎯 GEARS vocabulary: {len(dataset_genes):,} genes (from {vocab_source})")

    # Map your cardiac KG to GEARS vocabulary
    cardiac_go_aligned = create_proper_cardiac_go_alignment(
        cardiac_kg_path,
        dataset_genes,
        use_mygene=True
    )

    # Ensure required columns
    if 'importance' not in cardiac_go_aligned.columns:
        if 'weight' in cardiac_go_aligned.columns:
            cardiac_go_aligned = cardiac_go_aligned.rename(columns={'weight': 'importance'})
        else:
            cardiac_go_aligned['importance'] = 1.0

    # Save aligned cardiac KG for GEARS
    output_paths = [
        f"./data/{dataset_name}/go_essential_{dataset_name}.csv",
        f"./data/go_essential_{dataset_name}.csv"
    ]

    for output_path in output_paths:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        cardiac_go_aligned.to_csv(output_path, index=False)
        print(f"💾 Saved aligned cardiac KG: {output_path}")

    # Verify no vocabulary mismatches remain
    final_genes = set(cardiac_go_aligned['source']) | set(cardiac_go_aligned['target'])
    dataset_genes_set = set(dataset_genes)
    remaining_mismatches = final_genes - dataset_genes_set

    if remaining_mismatches:
        print(f"❌ WARNING: {len(remaining_mismatches)} genes still not in GEARS vocab:")
        print(f"   {sorted(list(remaining_mismatches))[:10]}")
        return None

    print(f"✅ SUCCESS: All {len(final_genes):,} genes in cardiac KG are in GEARS vocabulary")

    return pert, cardiac_go_aligned

# Main execution
if __name__ == "__main__":
    try:
        pert, aligned_kg = upload_cardiac_kg_to_gears()

        if pert is not None:
            print("\n🎉 YOUR CARDIAC KNOWLEDGE GRAPH IS NOW READY FOR GEARS!")
            print("=" * 60)
            print(f"📊 Aligned cardiac KG: {len(aligned_kg):,} edges")
            print(f"🧬 Genes covered: {len(set(aligned_kg['source']) | set(aligned_kg['target'])):,}")
            print("🚀 You can now train GEARS with your cardiac-specific biology!")
        else:
            print("❌ Failed to upload cardiac knowledge graph")

    except Exception as e:
        print(f"❌ Error: {e}")
        raise

#!/usr/bin/env python3
# Fix GEARS perturbation filtering issue

# -------- USER SETTINGS --------
BASE          = "/content/drive/MyDrive/iPSC_CM/CM4AI_KG"
DATASET_NAME  = "kolf2"
BATCH_SIZE    = 32
TEST_BSZ      = 128
SEED          = 1
EPOCHS        = 20
LR            = 1e-3
WEIGHT_DECAY  = 1e-4
# --------------------------------

import os, pandas as pd
from pathlib import Path

# Setup
BASE = os.path.abspath(BASE)
os.chdir(BASE)

print("🔧 FIXING GEARS PERTURBATION FILTERING ISSUE")
print("=" * 55)

# Import GEARS
try:
    from gears import PertData, GEARS
except Exception:
    import subprocess
    import sys
    subprocess.run([sys.executable, "-m", "pip", "install", "-q", "cell-gears"], check=True)
    from gears import PertData, GEARS

def device_auto():
    try:
        import torch
        return "cuda" if torch.cuda.is_available() else "cpu"
    except Exception:
        return "cpu"

def diagnose_perturbation_issue():
    """
    Diagnose why certain perturbations are not in the GO graph
    """
    print("🔍 Diagnosing perturbation filtering issue...")

    # Load dataset
    pert = PertData("./data", default_pert_graph=True)
    pert.load(data_path=f"./data/{DATASET_NAME}")

    print(f"📊 Dataset info:")
    print(f"   Cells: {pert.adata.n_obs:,}")
    print(f"   Genes: {pert.adata.n_vars:,}")

    # Check perturbation names in the dataset
    if 'condition' in pert.adata.obs.columns:
        conditions = pert.adata.obs['condition'].unique()
        print(f"   Unique conditions: {len(conditions)}")
        print(f"   Example conditions: {conditions[:10]}")

        # Check for problematic perturbations
        problematic_perts = [c for c in conditions if 'PRKAA2' in str(c) or 'HDAC11' in str(c)]
        print(f"   Problematic perturbations found: {problematic_perts}")

        # Extract gene names from perturbation conditions
        gene_names_in_conditions = set()
        for condition in conditions:
            if '+' in str(condition):
                # Split perturbations like "GENE1+GENE2" or "GENE+ctrl"
                parts = str(condition).split('+')
                for part in parts:
                    part = part.strip()
                    if part != 'ctrl' and part.upper() != 'CTRL':
                        gene_names_in_conditions.add(part)
            elif str(condition) not in ['ctrl', 'CTRL']:
                gene_names_in_conditions.add(str(condition))

        print(f"   Genes in perturbation conditions: {len(gene_names_in_conditions)}")
        print(f"   Example genes from conditions: {sorted(list(gene_names_in_conditions))[:10]}")

        # Check which genes are in the dataset vocabulary
        if hasattr(pert, 'adata'):
            dataset_genes = set(pert.adata.var_names)
            missing_genes = gene_names_in_conditions - dataset_genes

            print(f"   Genes in conditions but NOT in dataset: {len(missing_genes)}")
            if missing_genes:
                print(f"   Examples: {sorted(list(missing_genes))[:10]}")

        return conditions, gene_names_in_conditions
    else:
        print("   ⚠️ No 'condition' column found in adata.obs")
        return [], set()

def filter_to_valid_perturbations(conditions, gene_names_in_conditions):
    """
    Filter dataset to only include perturbations that can be processed by GEARS
    """
    print(f"\n🔧 Filtering to valid perturbations...")

    # Load dataset
    pert = PertData("./data", default_pert_graph=True)
    pert.load(data_path=f"./data/{DATASET_NAME}")

    # Get genes that are actually in the dataset
    dataset_genes = set(pert.adata.var_names)

    # Find valid perturbations (where all genes are in dataset)
    valid_conditions = []
    invalid_conditions = []

    for condition in conditions:
        condition_str = str(condition)
        is_valid = True

        if '+' in condition_str:
            # Multi-gene perturbation
            parts = condition_str.split('+')
            for part in parts:
                part = part.strip()
                if part not in ['ctrl', 'CTRL'] and part not in dataset_genes:
                    is_valid = False
                    break
        elif condition_str not in ['ctrl', 'CTRL']:
            # Single gene perturbation
            if condition_str not in dataset_genes:
                is_valid = False

        if is_valid:
            valid_conditions.append(condition)
        else:
            invalid_conditions.append(condition)

    print(f"   Valid conditions: {len(valid_conditions)}")
    print(f"   Invalid conditions: {len(invalid_conditions)}")

    if invalid_conditions:
        print(f"   Examples of invalid: {invalid_conditions[:10]}")

    # Filter the dataset to only valid conditions
    if len(valid_conditions) > 0:
        mask = pert.adata.obs['condition'].isin(valid_conditions)
        filtered_adata = pert.adata[mask].copy()

        print(f"   Filtered dataset: {filtered_adata.n_obs:,} cells")
        print(f"   Remaining conditions: {len(filtered_adata.obs['condition'].unique())}")

        # Create new PertData object and process it properly
        print(f"   📊 Creating new processed dataset...")

        # Create new PertData and process the filtered data
        pert_new = PertData("./data", default_pert_graph=True)

        # Use GEARS' data processing pipeline
        filtered_dataset_name = f"{DATASET_NAME}_filtered"
        pert_new.new_data_process(dataset_name=filtered_dataset_name, adata=filtered_adata)

        print(f"   ✅ New dataset processed: {filtered_dataset_name}")

        return filtered_dataset_name
    else:
        print("   ❌ No valid conditions found!")
        return None

def train_gears_with_filtered_data(filtered_dataset_name):
    """
    Train GEARS with the filtered dataset
    """
    print(f"\n🚀 Training GEARS with filtered data...")

    # Load filtered dataset
    pert = PertData("./data", default_pert_graph=True)
    pert.load(data_path=f"./data/{filtered_dataset_name}")

    print(f"✅ Filtered dataset loaded:")
    print(f"   📱 Cells: {pert.adata.n_obs:,}")
    print(f"   🧬 Genes: {pert.adata.n_vars:,}")
    print(f"   🔗 Conditions: {len(pert.adata.obs['condition'].unique())}")

    # Prepare splits
    print(f"\n⚙️ Preparing data splits...")
    try:
        pert.prepare_split(split="no_test", seed=SEED)
    except:
        pert.prepare_split(split="simulation", seed=SEED)

    pert.get_dataloader(batch_size=BATCH_SIZE, test_batch_size=TEST_BSZ)
    print("✅ Data splits and loaders ready")

    # Initialize and train model
    print(f"\n🧠 Initializing GEARS model...")
    device = device_auto()
    gears_model = GEARS(pert, device=device)
    gears_model.model_initialize(hidden_size=64)
    print("✅ GEARS model initialized!")

    print(f"\n🚀 Training...")
    try:
        gears_model.train(epochs=EPOCHS, lr=LR, weight_decay=WEIGHT_DECAY)
        print("🎊 Training completed successfully!")

        # Save model
        model_name = f"gears_filtered_{DATASET_NAME}"
        gears_model.save_model(model_name)
        print(f"💾 Model saved as: {model_name}")

        return gears_model

    except Exception as e:
        print(f"❌ Training failed: {e}")
        return None

def create_gene_alias_mapping():
    """
    Create mapping for common gene aliases that might be causing issues
    """
    print(f"\n🔄 Creating gene alias mapping...")

    # Common aliases for genes in your perturbation list
    aliases = {
        'PRKAA2': ['AMPKA2', 'AMPK'],
        'MSK2': ['RPS6KA4'],
        'MSK1': ['RPS6KA1'],
        'BRG1': ['SMARCA4'],
        'KAT3B': ['EP300', 'p300'],
        'KAT3A': ['CREBBP', 'CBP'],
        'ARDT1': ['PARP1', 'ARTD1'],
        'BRE1A': ['RNF20'],
    }

    # Load dataset to check which aliases exist
    pert = PertData("./data", default_pert_graph=True)
    pert.load(data_path=f"./data/{DATASET_NAME}")
    dataset_genes = set(pert.adata.var_names)

    working_mapping = {}
    for original, alias_list in aliases.items():
        for alias in alias_list:
            if alias in dataset_genes:
                working_mapping[original] = alias
                print(f"   ✅ {original} -> {alias}")
                break
        if original not in working_mapping and original in dataset_genes:
            working_mapping[original] = original
            print(f"   ✅ {original} -> {original} (exact match)")

    return working_mapping

def main():
    """
    Main function to fix the perturbation filtering issue
    """
    # Diagnose the issue
    conditions, gene_names = diagnose_perturbation_issue()

    if len(conditions) > 0:
        # Try to filter to valid perturbations
        filtered_dataset_name = filter_to_valid_perturbations(conditions, gene_names)

        if filtered_dataset_name:
            # Train with filtered data
            model = train_gears_with_filtered_data(filtered_dataset_name)

            if model:
                print(f"\n🎉 SUCCESS!")
                print(f"✅ GEARS trained with filtered perturbations")
                print(f"📊 Model ready for predictions")
                print(f"🔬 Dataset: {filtered_dataset_name}")
            else:
                print(f"\n❌ Training failed even with filtered data")
        else:
            print(f"\n❌ Could not create valid filtered dataset")

            # Show gene alias mapping as alternative
            create_gene_alias_mapping()
            print(f"\n💡 Consider updating your perturbation names using the aliases above")
    else:
        print(f"\n❌ Could not analyze perturbations")

if __name__ == "__main__":
    main()

# Load your trained model
gears_model.load_pretrained('gears_filtered_kolf2')

#!/usr/bin/env python3
# Comprehensive Perturbation Simulation System - Generate organized tables and results

import os, pandas as pd, numpy as np
from pathlib import Path
from datetime import datetime
import pickle

# -------- USER SETTINGS --------
BASE = "/content/drive/MyDrive/iPSC_CM/CM4AI_KG"
DATASET_NAME = "kolf2_filtered"
MODEL_NAME = "gears_filtered_kolf2"
OUTPUT_FOLDER = "perturbation_simulations"

# Define your perturbation experiments
PERTURBATION_EXPERIMENTS = {
    "cardiac_development": {
        "description": "Cardiac muscle development and function",
        "perturbations": [
            ["MYH7"],           # Cardiac myosin
            ["TPM1"],           # Tropomyosin
            ["TTN"],            # Titin
            ["MYH7", "TPM1"],   # Cardiac combo
        ]
    },
    "transcription_factors": {
        "description": "Master transcriptional regulators",
        "perturbations": [
            ["MEF2C"],          # Cardiac TF
            ["GATA4"],          # Cardiac TF
            ["TBX5"],           # Heart/limb TF
            ["MEF2C", "GATA4"], # TF combo
        ]
    },
    "epigenetic_regulators": {
        "description": "Chromatin modification and epigenetic control",
        "perturbations": [
            ["KDM6A"],          # Histone demethylase
            ["HDAC8"],          # Histone deacetylase
            ["HDAC9"],          # Histone deacetylase
            ["KAT2B"],          # Histone acetyltransferase
        ]
    },
    "cell_cycle_control": {
        "description": "Cell division and DNA damage response",
        "perturbations": [
            ["ATR"],            # DNA damage checkpoint
            ["AURKA"],          # Aurora kinase A
            ["AURKB"],          # Aurora kinase B
            ["CDKN1A"],         # p21, CDK inhibitor
        ]
    },
    "metabolism_signaling": {
        "description": "Metabolic regulation and signaling pathways",
        "perturbations": [
            ["PPARG"],          # Metabolic regulator
            ["PRKAA2"],         # AMPK subunit
            ["NODAL"],          # TGF-beta signaling
            ["FGF2"],           # Growth factor
        ]
    }
}
# --------------------------------

def setup_simulation_environment():
    """
    Set up the environment and load the trained GEARS model
    """
    print("🔧 SETTING UP PERTURBATION SIMULATION ENVIRONMENT")
    print("=" * 60)

    os.chdir(BASE)

    # Create output directory with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = Path(OUTPUT_FOLDER) / f"simulation_{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)

    print(f"📁 Output directory: {output_dir}")

    # Load GEARS
    from gears import PertData, GEARS

    print("📊 Loading dataset and model...")
    pert = PertData("./data", default_pert_graph=True)
    pert.load(data_path=f"./data/{DATASET_NAME}")

    print(f"   Dataset: {pert.adata.n_obs:,} cells, {pert.adata.n_vars:,} genes")

    # CRITICAL: Prepare the dataloader (this was missing!)
    print("⚙️ Preparing data splits and loaders...")
    try:
        pert.prepare_split(split="no_test", seed=1)
    except:
        pert.prepare_split(split="simulation", seed=1)

    pert.get_dataloader(batch_size=32, test_batch_size=128)
    print("   ✅ Dataloaders prepared")

    # Load trained model
    device = "cuda" if os.system("nvidia-smi > /dev/null 2>&1") == 0 else "cpu"
    gears_model = GEARS(pert, device=device)
    gears_model.load_pretrained(MODEL_NAME)

    print(f"   Model: {MODEL_NAME} loaded on {device}")
    print(f"✅ Environment ready!")

    return pert, gears_model, output_dir

def run_perturbation_experiment(gears_model, pert, experiment_name, experiment_config, output_dir):
    """
    Run a single perturbation experiment and save organized results
    """
    print(f"\n🧬 Running Experiment: {experiment_name.upper()}")
    print("=" * 50)
    print(f"Description: {experiment_config['description']}")

    # Create experiment subdirectory
    exp_dir = output_dir / experiment_name
    exp_dir.mkdir(exist_ok=True)

    perturbations = experiment_config['perturbations']
    results = {}

    # Get gene names for reference
    gene_names = pert.adata.var_names.tolist()

    for i, perturbation in enumerate(perturbations):
        pert_name = "+".join(perturbation)
        print(f"   🎯 Simulating: {pert_name}")

        try:
            # Make prediction
            prediction = gears_model.predict([perturbation])

            # Extract prediction data
            if isinstance(prediction, dict) and pert_name in prediction:
                pred_values = prediction[pert_name]
            elif isinstance(prediction, dict):
                # Sometimes GEARS returns with different key format
                key = list(prediction.keys())[0]
                pred_values = prediction[key]
            else:
                pred_values = prediction

            # Convert to numpy array if needed
            if hasattr(pred_values, 'detach'):
                pred_values = pred_values.detach().cpu().numpy()
            elif not isinstance(pred_values, np.ndarray):
                pred_values = np.array(pred_values)

            # Store results
            results[pert_name] = {
                'genes_perturbed': perturbation,
                'prediction_values': pred_values,
                'n_genes': len(pred_values)
            }

            print(f"      ✅ Success: {len(pred_values)} gene predictions")

        except Exception as e:
            print(f"      ❌ Failed: {e}")
            results[pert_name] = None

    # Create comprehensive results tables
    create_experiment_tables(results, gene_names, exp_dir, experiment_name, experiment_config)

    return results

def create_experiment_tables(results, gene_names, exp_dir, experiment_name, experiment_config):
    """
    Create organized tables and visualizations for the experiment
    """
    print(f"   📊 Creating result tables...")

    # 1. Master results table
    master_data = []
    prediction_matrix = []
    perturbation_names = []

    for pert_name, result in results.items():
        if result is not None:
            perturbation_names.append(pert_name)
            prediction_matrix.append(result['prediction_values'])

    if prediction_matrix:
        # Convert to DataFrame
        pred_df = pd.DataFrame(
            np.array(prediction_matrix).T,
            columns=perturbation_names,
            index=gene_names
        )

        # Save raw predictions
        pred_df.to_csv(exp_dir / f"{experiment_name}_raw_predictions.csv")

        # 2. Top changed genes table
        create_top_changed_genes_table(pred_df, exp_dir, experiment_name)

        # 3. Perturbation summary table
        create_perturbation_summary(results, exp_dir, experiment_name, experiment_config)

        # 4. Gene ranking tables
        create_gene_ranking_tables(pred_df, exp_dir, experiment_name)

        print(f"      ✅ Tables saved to {exp_dir}")

    else:
        print(f"      ❌ No successful predictions to save")

def create_top_changed_genes_table(pred_df, exp_dir, experiment_name):
    """
    Create table of top up/down regulated genes for each perturbation
    """
    top_genes_data = []

    for perturbation in pred_df.columns:
        pred_values = pred_df[perturbation]

        # Get top upregulated genes
        top_up_idx = pred_values.nlargest(20).index
        top_up_values = pred_values.nlargest(20).values

        # Get top downregulated genes
        top_down_idx = pred_values.nsmallest(20).index
        top_down_values = pred_values.nsmallest(20).values

        # Create summary
        for i in range(20):
            top_genes_data.append({
                'perturbation': perturbation,
                'rank': i + 1,
                'top_upregulated_gene': top_up_idx[i],
                'upregulation_score': top_up_values[i],
                'top_downregulated_gene': top_down_idx[i],
                'downregulation_score': top_down_values[i]
            })

    top_genes_df = pd.DataFrame(top_genes_data)
    top_genes_df.to_csv(exp_dir / f"{experiment_name}_top_changed_genes.csv", index=False)

def create_perturbation_summary(results, exp_dir, experiment_name, experiment_config):
    """
    Create high-level summary of perturbation effects
    """
    summary_data = []

    for pert_name, result in results.items():
        if result is not None:
            pred_values = result['prediction_values']

            summary = {
                'perturbation_name': pert_name,
                'genes_perturbed': ", ".join(result['genes_perturbed']),
                'total_genes_affected': len(pred_values),
                'mean_effect': np.mean(pred_values),
                'std_effect': np.std(pred_values),
                'max_upregulation': np.max(pred_values),
                'max_downregulation': np.min(pred_values),
                'genes_upregulated_>0.1': np.sum(pred_values > 0.1),
                'genes_downregulated_<-0.1': np.sum(pred_values < -0.1),
                'effect_magnitude': np.mean(np.abs(pred_values))
            }
            summary_data.append(summary)

    if summary_data:
        summary_df = pd.DataFrame(summary_data)
        summary_df.to_csv(exp_dir / f"{experiment_name}_summary.csv", index=False)

        # Add experiment metadata
        metadata = {
            'experiment_name': experiment_name,
            'description': experiment_config['description'],
            'timestamp': datetime.now().isoformat(),
            'n_perturbations': len(summary_data),
            'successful_predictions': len([r for r in results.values() if r is not None])
        }

        with open(exp_dir / f"{experiment_name}_metadata.txt", 'w') as f:
            for key, value in metadata.items():
                f.write(f"{key}: {value}\n")

def create_gene_ranking_tables(pred_df, exp_dir, experiment_name):
    """
    Create gene ranking tables for easy analysis
    """
    for perturbation in pred_df.columns:
        # Create ranking table for this perturbation
        gene_ranking = pd.DataFrame({
            'gene_name': pred_df.index,
            'predicted_change': pred_df[perturbation],
            'absolute_change': np.abs(pred_df[perturbation])
        })

        # Sort by absolute change (most affected genes first)
        gene_ranking = gene_ranking.sort_values('absolute_change', ascending=False)
        gene_ranking['rank'] = range(1, len(gene_ranking) + 1)

        # Add effect direction
        gene_ranking['effect_direction'] = gene_ranking['predicted_change'].apply(
            lambda x: 'UP' if x > 0 else 'DOWN'
        )

        # Save individual perturbation ranking
        safe_name = perturbation.replace('+', '_plus_').replace('/', '_')
        gene_ranking.to_csv(exp_dir / f"gene_ranking_{safe_name}.csv", index=False)

def create_master_summary_report(all_results, output_dir):
    """
    Create a master summary report across all experiments
    """
    print(f"\n📋 Creating master summary report...")

    # Collect all experiment summaries
    master_summary = []

    for exp_name, exp_results in all_results.items():
        if exp_results:
            n_successful = len([r for r in exp_results.values() if r is not None])
            n_total = len(exp_results)

            master_summary.append({
                'experiment_name': exp_name,
                'total_perturbations': n_total,
                'successful_predictions': n_successful,
                'success_rate': f"{n_successful/n_total*100:.1f}%"
            })

    # Save master summary
    master_df = pd.DataFrame(master_summary)
    master_df.to_csv(output_dir / "master_experiment_summary.csv", index=False)

    # Create README file
    create_results_readme(output_dir, all_results)

def create_results_readme(output_dir, all_results):
    """
    Create a README file explaining the results structure
    """
    readme_content = f"""
# Perturbation Simulation Results

Generated on: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
Model: {MODEL_NAME}
Dataset: {DATASET_NAME}

## Folder Structure

Each experiment folder contains:
- `[experiment]_raw_predictions.csv`: Full gene expression predictions matrix
- `[experiment]_summary.csv`: High-level statistics for each perturbation
- `[experiment]_top_changed_genes.csv`: Top 20 up/down regulated genes
- `[experiment]_metadata.txt`: Experiment details and parameters
- `gene_ranking_[perturbation].csv`: All genes ranked by effect magnitude

## File Descriptions

### Raw Predictions
- Rows: Genes (6,000 total)
- Columns: Perturbations
- Values: Predicted log fold change in gene expression

### Summary Files
- Mean/std effects across all genes
- Count of significantly up/down regulated genes
- Effect magnitude metrics

### Gene Rankings
- Individual files for each perturbation
- Genes sorted by absolute effect size
- Effect direction (UP/DOWN) indicated

## Experiments Run

"""

    for exp_name, exp_config in PERTURBATION_EXPERIMENTS.items():
        readme_content += f"### {exp_name}\n"
        readme_content += f"- Description: {exp_config['description']}\n"
        readme_content += f"- Perturbations: {len(exp_config['perturbations'])}\n"

        if exp_name in all_results:
            successful = len([r for r in all_results[exp_name].values() if r is not None])
            readme_content += f"- Successful predictions: {successful}\n"

        readme_content += "\n"

    readme_content += """
## Usage Tips

1. Start with the summary files for overview
2. Use gene ranking files to find most affected genes
3. Cross-reference with raw predictions for detailed analysis
4. Compare effects across different perturbations

## Next Steps

- Validate top predicted genes experimentally
- Perform pathway enrichment on affected gene sets
- Compare predictions with literature knowledge
- Design follow-up experiments based on results
"""

    with open(output_dir / "README.md", 'w') as f:
        f.write(readme_content)

def main():
    """
    Main function to run all perturbation simulations
    """
    print("🧬 COMPREHENSIVE PERTURBATION SIMULATION SYSTEM")
    print("=" * 65)

    # Setup environment
    pert, gears_model, output_dir = setup_simulation_environment()

    # Run all experiments
    all_results = {}

    for exp_name, exp_config in PERTURBATION_EXPERIMENTS.items():
        results = run_perturbation_experiment(
            gears_model, pert, exp_name, exp_config, output_dir
        )
        all_results[exp_name] = results

    # Create master summary
    create_master_summary_report(all_results, output_dir)

    # Final summary
    print(f"\n🎉 SIMULATION COMPLETE!")
    print("=" * 30)
    print(f"📁 Results saved to: {output_dir}")
    print(f"📊 Experiments run: {len(PERTURBATION_EXPERIMENTS)}")

    total_perturbations = sum(len(config['perturbations']) for config in PERTURBATION_EXPERIMENTS.values())
    successful_predictions = sum(
        len([r for r in results.values() if r is not None])
        for results in all_results.values()
    )

    print(f"🎯 Total perturbations: {total_perturbations}")
    print(f"✅ Successful predictions: {successful_predictions}")
    print(f"📈 Success rate: {successful_predictions/total_perturbations*100:.1f}%")

    print(f"\n📋 Check the README.md file for detailed instructions!")

if __name__ == "__main__":
    main()

#!/usr/bin/env python3
# GEARS Perturbation Simulation - Apply perturbation vectors to cells and visualize before/after

import os, pandas as pd, numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# -------- USER SETTINGS --------
BASE = "/content/drive/MyDrive/iPSC_CM/CM4AI_KG"
DATASET_NAME = "kolf2_filtered"
MODEL_NAME = "gears_filtered_kolf2"
OUTPUT_FOLDER = "perturbation_effects_analysis"

# Select perturbations to simulate
PERTURBATIONS_TO_TEST = [
    ['MEF2C'],           # Cardiac transcription factor
    ['GATA4'],           # Cardiac transcription factor
    ['TBX5'],            # Heart/limb transcription factor
    ['MEF2C', 'GATA4'],  # Combination
    ['HDAC8'],           # Chromatin modifier
    ['KDM6A'],           # Chromatin modifier
    ['ATR'],             # Cell cycle
    ['AURKA'],           # Cell cycle
    ['TPM1'],            # Cardiac structural
]

N_CELLS_TO_SIMULATE = 1000  # Number of cells to simulate perturbation on
# --------------------------------

def setup_environment():
    """
    Set up environment and load GEARS model
    """
    print("🔧 SETTING UP GEARS PERTURBATION SIMULATION")
    print("=" * 50)

    # Install packages
    packages = ["umap-learn", "plotly", "scikit-learn"]
    for package in packages:
        try:
            __import__(package.replace('-', '_'))
        except ImportError:
            print(f"Installing {package}...")
            os.system(f"pip install {package}")

    # Import required libraries
    global umap, px, go, make_subplots, PCA, StandardScaler
    import umap
    import plotly.express as px
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots
    from sklearn.decomposition import PCA
    from sklearn.preprocessing import StandardScaler

    os.chdir(BASE)

    # Create output directory
    output_dir = Path(OUTPUT_FOLDER)
    output_dir.mkdir(exist_ok=True)

    print(f"✅ Environment ready! Output: {output_dir}")
    return output_dir

def load_gears_model():
    """
    Load the trained GEARS model and dataset
    """
    print("\n📊 LOADING GEARS MODEL AND DATASET")
    print("=" * 40)

    from gears import PertData, GEARS

    # Load dataset
    pert = PertData("./data", default_pert_graph=True)
    pert.load(data_path=f"./data/{DATASET_NAME}")

    # Prepare splits and dataloaders (required for GEARS)
    pert.prepare_split(split="no_test", seed=1)
    pert.get_dataloader(batch_size=32, test_batch_size=128)

    print(f"   📱 Dataset: {pert.adata.n_obs:,} cells, {pert.adata.n_vars:,} genes")

    # Load trained model
    device = "cuda" if os.system("nvidia-smi > /dev/null 2>&1") == 0 else "cpu"
    gears_model = GEARS(pert, device=device)
    gears_model.load_pretrained(MODEL_NAME)

    print(f"   🧠 Model: {MODEL_NAME} loaded on {device}")
    print("✅ GEARS model ready for perturbation simulation!")

    return pert, gears_model

def get_baseline_cells(pert, n_cells=1000):
    """
    Get baseline (unperturbed) cells for simulation
    """
    print(f"\n📱 SELECTING BASELINE CELLS")
    print("=" * 30)

    # Get control cells (unperturbed)
    ctrl_mask = pert.adata.obs['condition'] == 'ctrl'
    ctrl_cells = pert.adata[ctrl_mask].copy()

    print(f"   Available control cells: {ctrl_cells.n_obs:,}")

    # Sample subset if we have more than needed
    if ctrl_cells.n_obs > n_cells:
        import random
        random.seed(42)
        indices = random.sample(range(ctrl_cells.n_obs), n_cells)
        ctrl_cells = ctrl_cells[indices].copy()
        print(f"   Sampled to: {ctrl_cells.n_obs:,} cells")

    # Get gene expression matrix (cells × genes)
    if hasattr(ctrl_cells.X, 'toarray'):
        baseline_expression = ctrl_cells.X.toarray()
    else:
        baseline_expression = np.array(ctrl_cells.X)

    gene_names = ctrl_cells.var_names.tolist()

    print(f"   ✅ Baseline expression: {baseline_expression.shape[0]} cells × {baseline_expression.shape[1]} genes")

    return baseline_expression, gene_names, ctrl_cells

def apply_perturbation_to_cells(gears_model, perturbation, baseline_expression, gene_names):
    """
    Apply GEARS perturbation prediction to baseline cells
    """
    print(f"   🎯 Applying perturbation: {'+'.join(perturbation)}")

    try:
        # Get perturbation prediction from GEARS
        perturbation_pred = gears_model.predict([perturbation])

        # Extract perturbation vector
        pert_key = list(perturbation_pred.keys())[0]
        pert_vector = perturbation_pred[pert_key]

        # Convert to numpy if needed
        if hasattr(pert_vector, 'detach'):
            pert_vector = pert_vector.detach().cpu().numpy()
        elif not isinstance(pert_vector, np.ndarray):
            pert_vector = np.array(pert_vector)

        print(f"      📊 Perturbation vector shape: {pert_vector.shape}")

        # Apply perturbation to all baseline cells
        # GEARS prediction is log fold change, so we add it to log-transformed baseline
        log_baseline = np.log1p(baseline_expression)  # log(1 + x) transformation

        # Add perturbation vector to each cell
        perturbed_log = log_baseline + pert_vector[np.newaxis, :]  # Broadcasting

        # Convert back to linear space
        perturbed_expression = np.expm1(perturbed_log)  # exp(x) - 1, inverse of log1p

        # Ensure non-negative values
        perturbed_expression = np.maximum(perturbed_expression, 0)

        print(f"      ✅ Applied to {perturbed_expression.shape[0]} cells")

        return perturbed_expression, pert_vector

    except Exception as e:
        print(f"      ❌ Failed: {e}")
        return None, None

def simulate_all_perturbations(gears_model, perturbations, baseline_expression, gene_names):
    """
    Simulate all perturbations and collect results
    """
    print(f"\n🧪 SIMULATING {len(perturbations)} PERTURBATIONS")
    print("=" * 45)

    simulation_results = {
        'baseline': {
            'expression': baseline_expression,
            'label': 'Baseline (Control)',
            'perturbation': ['ctrl']
        }
    }

    for i, perturbation in enumerate(perturbations):
        pert_name = '+'.join(perturbation)
        print(f"   {i+1}/{len(perturbations)}: {pert_name}")

        perturbed_expr, pert_vector = apply_perturbation_to_cells(
            gears_model, perturbation, baseline_expression, gene_names
        )

        if perturbed_expr is not None:
            simulation_results[pert_name] = {
                'expression': perturbed_expr,
                'label': f'Perturbed: {pert_name}',
                'perturbation': perturbation,
                'perturbation_vector': pert_vector
            }

    print(f"✅ Simulation complete: {len(simulation_results)} conditions")
    return simulation_results

def create_before_after_umap(simulation_results, output_dir, gene_names):
    """
    Create UMAP visualization of before/after perturbation effects
    """
    print(f"\n🗺️ CREATING BEFORE/AFTER UMAP VISUALIZATION")
    print("=" * 45)

    # Combine all expression data
    all_expressions = []
    all_labels = []
    all_conditions = []

    for condition, data in simulation_results.items():
        expression = data['expression']
        label = data['label']

        all_expressions.append(expression)
        all_labels.extend([label] * expression.shape[0])
        all_conditions.extend([condition] * expression.shape[0])

    # Combine into single matrix
    combined_expression = np.vstack(all_expressions)
    print(f"   📊 Combined data: {combined_expression.shape[0]} cells × {combined_expression.shape[1]} genes")

    # Log transform and scale for UMAP
    log_expr = np.log1p(combined_expression)

    # Optional: Select highly variable genes for better UMAP
    gene_vars = np.var(log_expr, axis=0)
    top_var_indices = np.argsort(gene_vars)[-2000:]  # Top 2000 most variable genes
    log_expr_hvg = log_expr[:, top_var_indices]

    print(f"   🧬 Using top {len(top_var_indices)} variable genes for UMAP")

    # Scale the data
    scaler = StandardScaler()
    scaled_expr = scaler.fit_transform(log_expr_hvg)

    # Compute UMAP
    print("   🔄 Computing UMAP embedding...")
    reducer = umap.UMAP(
        n_neighbors=30,
        min_dist=0.3,
        n_components=2,
        random_state=42,
        metric='euclidean'
    )

    umap_embedding = reducer.fit_transform(scaled_expr)
    print("   ✅ UMAP embedding complete")

    # Create interactive plot
    df_plot = pd.DataFrame({
        'UMAP1': umap_embedding[:, 0],
        'UMAP2': umap_embedding[:, 1],
        'Condition': all_labels,
        'Condition_ID': all_conditions
    })

    # Create plotly visualization
    fig = px.scatter(
        df_plot,
        x='UMAP1',
        y='UMAP2',
        color='Condition',
        title='Cell State Changes: Before and After Perturbations',
        width=1000,
        height=700,
        opacity=0.6
    )

    fig.update_layout(
        font=dict(size=12),
        title_font_size=16,
        legend=dict(
            yanchor="top",
            y=0.99,
            xanchor="left",
            x=1.01
        )
    )

    # Save interactive plot
    fig.write_html(output_dir / "before_after_perturbations_umap.html")
    print(f"   ✅ Interactive UMAP saved: before_after_perturbations_umap.html")

    # Create separate comparison plots
    create_individual_comparisons(simulation_results, output_dir, umap_embedding, all_conditions)

    return umap_embedding, df_plot

def create_individual_comparisons(simulation_results, output_dir, umap_embedding, all_conditions):
    """
    Create individual before/after comparison plots
    """
    print("   📊 Creating individual comparison plots...")

    baseline_mask = np.array(all_conditions) == 'baseline'
    baseline_umap = umap_embedding[baseline_mask]

    # Create subplot figure
    n_perturbations = len(simulation_results) - 1  # Exclude baseline
    cols = 3
    rows = (n_perturbations + cols - 1) // cols

    fig = make_subplots(
        rows=rows, cols=cols,
        subplot_titles=[k for k in simulation_results.keys() if k != 'baseline'],
        specs=[[{"type": "scatter"}] * cols for _ in range(rows)]
    )

    plot_idx = 0
    for condition, data in simulation_results.items():
        if condition == 'baseline':
            continue

        plot_idx += 1
        row = ((plot_idx - 1) // cols) + 1
        col = ((plot_idx - 1) % cols) + 1

        # Get perturbation cells
        pert_mask = np.array(all_conditions) == condition
        pert_umap = umap_embedding[pert_mask]

        # Add baseline cells
        fig.add_trace(
            go.Scatter(
                x=baseline_umap[:, 0],
                y=baseline_umap[:, 1],
                mode='markers',
                marker=dict(color='lightgray', size=3, opacity=0.5),
                name='Baseline',
                showlegend=(plot_idx == 1)
            ),
            row=row, col=col
        )

        # Add perturbed cells
        fig.add_trace(
            go.Scatter(
                x=pert_umap[:, 0],
                y=pert_umap[:, 1],
                mode='markers',
                marker=dict(color='red', size=4, opacity=0.7),
                name=f'Perturbed: {condition}',
                showlegend=(plot_idx == 1)
            ),
            row=row, col=col
        )

    fig.update_layout(
        title="Individual Perturbation Effects on Cell States",
        height=200 * rows,
        showlegend=True
    )

    fig.write_html(output_dir / "individual_perturbation_comparisons.html")
    print(f"      ✅ Individual comparisons saved")

def analyze_perturbation_magnitudes(simulation_results, output_dir, gene_names):
    """
    Analyze the magnitude of perturbation effects
    """
    print(f"\n📈 ANALYZING PERTURBATION MAGNITUDES")
    print("=" * 40)

    baseline_expr = simulation_results['baseline']['expression']
    baseline_mean = np.mean(baseline_expr, axis=0)

    magnitude_results = []

    for condition, data in simulation_results.items():
        if condition == 'baseline':
            continue

        pert_expr = data['expression']
        pert_mean = np.mean(pert_expr, axis=0)

        # Calculate fold changes
        fold_changes = np.log2((pert_mean + 1) / (baseline_mean + 1))

        # Calculate magnitude metrics
        magnitude_results.append({
            'perturbation': condition,
            'mean_fold_change': np.mean(np.abs(fold_changes)),
            'max_upregulation': np.max(fold_changes),
            'max_downregulation': np.min(fold_changes),
            'genes_up_2fold': np.sum(fold_changes > 1),  # >2x upregulated
            'genes_down_2fold': np.sum(fold_changes < -1),  # >2x downregulated
            'total_affected_genes': np.sum(np.abs(fold_changes) > 0.5)  # >1.4x change
        })

    magnitude_df = pd.DataFrame(magnitude_results)
    magnitude_df.to_csv(output_dir / "perturbation_magnitude_analysis.csv", index=False)

    # Create magnitude visualization
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # Mean fold change
    sns.barplot(data=magnitude_df, x='perturbation', y='mean_fold_change', ax=axes[0,0])
    axes[0,0].set_title('Mean Absolute Fold Change')
    axes[0,0].tick_params(axis='x', rotation=45)

    # Max effects
    axes[0,1].scatter(magnitude_df['max_upregulation'], magnitude_df['max_downregulation'])
    axes[0,1].set_xlabel('Max Upregulation (log2 FC)')
    axes[0,1].set_ylabel('Max Downregulation (log2 FC)')
    axes[0,1].set_title('Range of Effects')

    # Number of affected genes
    sns.barplot(data=magnitude_df, x='perturbation', y='total_affected_genes', ax=axes[1,0])
    axes[1,0].set_title('Total Affected Genes (>1.4x change)')
    axes[1,0].tick_params(axis='x', rotation=45)

    # Up vs down regulation
    axes[1,1].scatter(magnitude_df['genes_up_2fold'], magnitude_df['genes_down_2fold'])
    axes[1,1].set_xlabel('Genes Upregulated >2x')
    axes[1,1].set_ylabel('Genes Downregulated >2x')
    axes[1,1].set_title('Up vs Down Regulation Balance')

    plt.tight_layout()
    plt.savefig(output_dir / "perturbation_magnitude_plots.png", dpi=300, bbox_inches='tight')
    plt.close()

    print(f"   ✅ Magnitude analysis saved")
    return magnitude_df

def save_simulation_data(simulation_results, output_dir, gene_names):
    """
    Save simulation data for further analysis
    """
    print(f"\n💾 SAVING SIMULATION DATA")
    print("=" * 25)

    # Save expression matrices
    for condition, data in simulation_results.items():
        expr_df = pd.DataFrame(
            data['expression'],
            columns=gene_names
        )
        expr_df['condition'] = condition
        expr_df['cell_id'] = [f"{condition}_{i}" for i in range(len(expr_df))]

        filename = f"expression_matrix_{condition.replace('+', '_plus_')}.csv"
        expr_df.to_csv(output_dir / filename, index=False)

    print(f"   ✅ Expression matrices saved")

    # Save perturbation vectors
    pert_vectors = {}
    for condition, data in simulation_results.items():
        if 'perturbation_vector' in data:
            pert_vectors[condition] = data['perturbation_vector'].tolist()

    if pert_vectors:
        import json
        with open(output_dir / "perturbation_vectors.json", 'w') as f:
            json.dump(pert_vectors, f, indent=2)
        print(f"   ✅ Perturbation vectors saved")

def main():
    """
    Main function to run perturbation simulation
    """
    print("🧬 GEARS PERTURBATION SIMULATION WITH BEFORE/AFTER ANALYSIS")
    print("=" * 70)

    # Setup environment
    output_dir = setup_environment()

    # Load GEARS model
    pert, gears_model = load_gears_model()

    # Get baseline cells
    baseline_expr, gene_names, ctrl_cells = get_baseline_cells(pert, N_CELLS_TO_SIMULATE)

    # Simulate all perturbations
    simulation_results = simulate_all_perturbations(
        gears_model, PERTURBATIONS_TO_TEST, baseline_expr, gene_names
    )

    # Create UMAP visualization
    umap_embedding, plot_df = create_before_after_umap(
        simulation_results, output_dir, gene_names
    )

    # Analyze perturbation magnitudes
    magnitude_df = analyze_perturbation_magnitudes(
        simulation_results, output_dir, gene_names
    )

    # Save all data
    save_simulation_data(simulation_results, output_dir, gene_names)

    # Final summary
    print(f"\n🎉 PERTURBATION SIMULATION COMPLETE!")
    print("=" * 45)
    print(f"📁 Results saved to: {output_dir}")
    print(f"🗺️ Interactive UMAP: before_after_perturbations_umap.html")
    print(f"📊 Individual comparisons: individual_perturbation_comparisons.html")
    print(f"📈 Magnitude analysis: perturbation_magnitude_plots.png")
    print(f"💾 Expression data: expression_matrix_*.csv files")

    print(f"\n🎯 Key Findings Preview:")
    if len(magnitude_df) > 0:
        strongest = magnitude_df.loc[magnitude_df['mean_fold_change'].idxmax()]
        print(f"   🏆 Strongest effect: {strongest['perturbation']}")
        print(f"   📊 Mean fold change: {strongest['mean_fold_change']:.3f}")
        print(f"   🧬 Affected genes: {strongest['total_affected_genes']}")

    print(f"\n🚀 Ready to explore cell state changes in the interactive plots!")

if __name__ == "__main__":
    main()

#!/usr/bin/env python3
# Quick viewer for your perturbation analysis results

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
from IPython.display import HTML, Image, Markdown, display

# Set paths
BASE = "/content/drive/MyDrive/iPSC_CM/CM4AI_KG"
ANALYSIS_DIR = f"{BASE}/comprehensive_analysis"

def view_interactive_umap():
    """
    Display the interactive UMAP visualization
    """
    print("🗺️ INTERACTIVE UMAP VISUALIZATION")
    print("=" * 40)

    umap_file = f"{ANALYSIS_DIR}/perturbation_umap_analysis.html"

    if os.path.exists(umap_file):
        print("✅ Loading interactive UMAP...")
        return HTML(umap_file)
    else:
        print("❌ UMAP file not found!")
        return None

def view_effect_plots():
    """
    Display the effect analysis plots
    """
    print("\n📈 EFFECT ANALYSIS PLOTS")
    print("=" * 30)

    plot_file = f"{ANALYSIS_DIR}/perturbation_effect_analysis.png"

    if os.path.exists(plot_file):
        print("✅ Loading effect analysis plots...")
        return Image(plot_file)
    else:
        print("❌ Effect plots not found!")
        return None

def view_validation_plots():
    """
    Display biological validation plots
    """
    print("\n🔬 BIOLOGICAL VALIDATION PLOTS")
    print("=" * 35)

    validation_file = f"{ANALYSIS_DIR}/biological_validation_plots.png"

    if os.path.exists(validation_file):
        print("✅ Loading validation plots...")
        return Image(validation_file)
    else:
        print("❌ Validation plots not found!")
        return None

def load_and_display_data():
    """
    Load and display key data tables
    """
    print("\n📊 DATA SUMMARY TABLES")
    print("=" * 25)

    os.chdir(ANALYSIS_DIR)

    # Load summary data
    if os.path.exists('combined_perturbation_summary.csv'):
        summary = pd.read_csv('combined_perturbation_summary.csv')
        print("📋 Perturbation Summary (Top 10 by Effect Magnitude):")
        top_effects = summary.nlargest(10, 'effect_magnitude')[
            ['perturbation_name', 'experiment', 'effect_magnitude', 'total_affected']
        ]
        display(top_effects)

    # Load validation data
    if os.path.exists('biological_validation_results.csv'):
        validation = pd.read_csv('biological_validation_results.csv')
        print("\n🔬 Biological Validation Summary:")

        # Show mean effects on known gene sets
        validation_summary = validation.groupby(['experiment', 'gene_set']).agg({
            'mean_effect': 'mean',
            'n_genes_available': 'mean'
        }).round(3)
        display(validation_summary)

    return summary if 'summary' in locals() else None, validation if 'validation' in locals() else None

def view_analysis_report():
    """
    Display the comprehensive analysis report
    """
    print("\n📖 COMPREHENSIVE ANALYSIS REPORT")
    print("=" * 40)

    report_file = f"{ANALYSIS_DIR}/ANALYSIS_REPORT.md"

    if os.path.exists(report_file):
        with open(report_file, 'r') as f:
            report_content = f.read()

        print("✅ Loading analysis report...")
        return Markdown(report_content)
    else:
        print("❌ Analysis report not found!")
        return None

def create_custom_visualization(summary_df):
    """
    Create custom visualization of your results
    """
    print("\n🎨 CUSTOM RESULT VISUALIZATION")
    print("=" * 35)

    if summary_df is None:
        print("❌ No summary data available")
        return

    # Create a custom plot
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # 1. Effect magnitude by experiment
    sns.barplot(data=summary_df, x='experiment', y='effect_magnitude', ax=axes[0,0])
    axes[0,0].set_title('Effect Magnitude by Experiment Type')
    axes[0,0].tick_params(axis='x', rotation=45)

    # 2. Most affected perturbations
    top_perts = summary_df.nlargest(8, 'effect_magnitude')
    sns.barplot(data=top_perts, x='perturbation_name', y='effect_magnitude', ax=axes[0,1])
    axes[0,1].set_title('Top 8 Perturbations by Effect Magnitude')
    axes[0,1].tick_params(axis='x', rotation=90)

    # 3. Total genes affected
    sns.scatterplot(data=summary_df, x='effect_magnitude', y='total_affected',
                   hue='experiment', s=100, ax=axes[1,0])
    axes[1,0].set_title('Effect Magnitude vs Genes Affected')
    axes[1,0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')

    # 4. Up vs Down regulation balance
    sns.scatterplot(data=summary_df, x='genes_upregulated_>0.1', y='genes_downregulated_<-0.1',
                   hue='experiment', s=100, ax=axes[1,1])
    axes[1,1].set_title('Upregulation vs Downregulation Balance')
    axes[1,1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')

    plt.tight_layout()
    plt.show()

    print("✅ Custom visualization complete!")

def show_file_list():
    """
    Show all available files in the analysis directory
    """
    print("\n📁 AVAILABLE ANALYSIS FILES")
    print("=" * 32)

    if os.path.exists(ANALYSIS_DIR):
        files = os.listdir(ANALYSIS_DIR)
        for file in sorted(files):
            file_path = f"{ANALYSIS_DIR}/{file}"
            size = os.path.getsize(file_path) / 1024  # KB
            print(f"   📄 {file} ({size:.1f} KB)")
    else:
        print("❌ Analysis directory not found!")

def main():
    """
    Main function to view all analysis results
    """
    print("🎯 PERTURBATION ANALYSIS RESULTS VIEWER")
    print("=" * 45)

    # Show available files
    show_file_list()

    # 1. View interactive UMAP
    umap_viz = view_interactive_umap()
    if umap_viz:
        display(umap_viz)

    # 2. View effect plots
    effect_plots = view_effect_plots()
    if effect_plots:
        display(effect_plots)

    # 3. View validation plots
    validation_plots = view_validation_plots()
    if validation_plots:
        display(validation_plots)

    # 4. Load and display data
    summary_df, validation_df = load_and_display_data()

    # 5. Create custom visualization
    if summary_df is not None:
        create_custom_visualization(summary_df)

    # 6. View analysis report
    report = view_analysis_report()
    if report:
        display(report)

    print("\n🎉 ANALYSIS RESULTS VIEWING COMPLETE!")
    print("💡 All visualizations and data are now displayed above")

if __name__ == "__main__":
    main()

#!/usr/bin/env python3
# Check and locate your saved GEARS model

import os
import glob
from pathlib import Path

BASE = "/content/drive/MyDrive/iPSC_CM/CM4AI_KG"
MODEL_NAME = "gears_filtered_kolf2"

def find_saved_models():
    """
    Search for saved GEARS models in your Google Drive
    """
    print("🔍 SEARCHING FOR SAVED GEARS MODELS")
    print("=" * 40)

    os.chdir(BASE)

    # Common locations where GEARS saves models
    search_locations = [
        ".",  # Current directory
        "./models",
        "./saved_models",
        f"./data/{MODEL_NAME}",
        "./data/kolf2_filtered",
        "./checkpoints"
    ]

    found_models = []

    print("📁 Searching in common locations...")

    for location in search_locations:
        if os.path.exists(location):
            print(f"   📂 Checking: {location}")

            # Look for model files (various extensions GEARS might use)
            patterns = [
                f"{MODEL_NAME}*",
                "*.pt", "*.pth", "*.pkl", "*.ckpt",
                "model*", "checkpoint*", "*gears*"
            ]

            for pattern in patterns:
                files = glob.glob(os.path.join(location, pattern))
                for file in files:
                    if os.path.isfile(file):
                        size = os.path.getsize(file) / (1024*1024)  # MB
                        found_models.append({
                            'path': os.path.abspath(file),
                            'name': os.path.basename(file),
                            'size_mb': size,
                            'location': location
                        })
                        print(f"      ✅ Found: {os.path.basename(file)} ({size:.1f} MB)")

    # Also search recursively for any GEARS-related files
    print(f"\n🔍 Recursive search for '{MODEL_NAME}'...")
    recursive_files = glob.glob(f"**/*{MODEL_NAME}*", recursive=True)

    for file in recursive_files:
        if os.path.isfile(file):
            abs_path = os.path.abspath(file)
            if not any(model['path'] == abs_path for model in found_models):
                size = os.path.getsize(file) / (1024*1024)
                found_models.append({
                    'path': abs_path,
                    'name': os.path.basename(file),
                    'size_mb': size,
                    'location': 'recursive_search'
                })
                print(f"   ✅ Found: {file} ({size:.1f} MB)")

    return found_models

def check_gears_model_structure():
    """
    Check if GEARS creates a specific model directory structure
    """
    print(f"\n📋 CHECKING GEARS MODEL DIRECTORY STRUCTURE")
    print("=" * 50)

    # GEARS might save models in a specific way
    # Let's check the current working directory structure
    current_files = os.listdir('.')

    print("📂 Files in current directory:")
    for item in sorted(current_files):
        if os.path.isfile(item):
            size = os.path.getsize(item) / (1024*1024)
            print(f"   📄 {item} ({size:.1f} MB)")
        elif os.path.isdir(item):
            print(f"   📁 {item}/")
            # Check inside directories for model files
            try:
                sub_files = os.listdir(item)
                model_files = [f for f in sub_files if any(ext in f.lower() for ext in ['.pt', '.pth', '.pkl', 'model', 'checkpoint'])]
                if model_files:
                    print(f"      🎯 Model files: {model_files}")
            except PermissionError:
                pass

def verify_model_loadability(model_paths):
    """
    Try to verify if found models can be loaded by GEARS
    """
    print(f"\n🧪 VERIFYING MODEL LOADABILITY")
    print("=" * 35)

    if not model_paths:
        print("❌ No models found to verify")
        return

    try:
        from gears import PertData, GEARS

        # Load the dataset first
        pert = PertData("./data", default_pert_graph=True)
        pert.load(data_path="./data/kolf2_filtered")
        pert.prepare_split(split="no_test", seed=1)
        pert.get_dataloader(batch_size=32, test_batch_size=128)

        print("✅ Dataset loaded successfully")

        # Try to create GEARS model
        device = "cuda" if os.system("nvidia-smi > /dev/null 2>&1") == 0 else "cpu"
        gears_model = GEARS(pert, device=device)

        print("✅ GEARS model object created")

        # Try to load the trained model
        try:
            gears_model.load_pretrained(MODEL_NAME)
            print(f"✅ Model '{MODEL_NAME}' loaded successfully!")
            print(f"   📊 Model is ready for predictions")
            return True
        except Exception as e:
            print(f"❌ Failed to load model '{MODEL_NAME}': {e}")

            # Try loading with full path
            for model_info in model_paths:
                if MODEL_NAME in model_info['name']:
                    try:
                        model_path_no_ext = model_info['path'].replace('.pt', '').replace('.pth', '').replace('.pkl', '')
                        gears_model.load_pretrained(model_path_no_ext)
                        print(f"✅ Model loaded from: {model_info['path']}")
                        return True
                    except Exception as e2:
                        print(f"❌ Failed to load from {model_info['path']}: {e2}")

            return False

    except Exception as e:
        print(f"❌ Error setting up GEARS environment: {e}")
        return False

def show_model_info(model_paths):
    """
    Display detailed information about found models
    """
    if not model_paths:
        print("\n❌ NO MODELS FOUND")
        print("The model might not have been saved successfully during training.")
        return

    print(f"\n📊 FOUND {len(model_paths)} MODEL FILE(S)")
    print("=" * 35)

    for i, model in enumerate(model_paths, 1):
        print(f"\n🎯 Model {i}:")
        print(f"   📄 Name: {model['name']}")
        print(f"   📁 Location: {model['location']}")
        print(f"   📂 Full path: {model['path']}")
        print(f"   💾 Size: {model['size_mb']:.1f} MB")

        # Check if this looks like the main model
        if MODEL_NAME in model['name'] and model['size_mb'] > 1:  # Reasonable size for a trained model
            print(f"   ⭐ This appears to be your main trained model!")

def create_model_backup_script(model_paths):
    """
    Create a script to backup your models
    """
    if not model_paths:
        return

    print(f"\n💾 MODEL BACKUP RECOMMENDATIONS")
    print("=" * 35)

    print("📋 To backup your models, you can:")
    print("1. Download them directly from Google Drive")
    print("2. Copy to a backup folder")
    print("3. Use the following Python code:")

    backup_code = f"""
# Backup your GEARS models
import shutil
import os

backup_dir = "{BASE}/model_backups"
os.makedirs(backup_dir, exist_ok=True)

models_to_backup = {[model['path'] for model in model_paths]}

for model_path in models_to_backup:
    if os.path.exists(model_path):
        backup_path = os.path.join(backup_dir, os.path.basename(model_path))
        shutil.copy2(model_path, backup_path)
        print(f"✅ Backed up: {{os.path.basename(model_path)}}")
"""

    print(f"```python{backup_code}```")

def main():
    """
    Main function to check for saved models
    """
    print("🔍 GEARS MODEL LOCATION CHECKER")
    print("=" * 35)
    print(f"🎯 Looking for model: {MODEL_NAME}")
    print(f"📁 Base directory: {BASE}")

    # Find all model files
    model_paths = find_saved_models()

    # Check directory structure
    check_gears_model_structure()

    # Show model information
    show_model_info(model_paths)

    # Verify model can be loaded
    if model_paths:
        model_works = verify_model_loadability(model_paths)
        if model_works:
            print(f"\n🎉 SUCCESS! Your model is saved and working!")
        else:
            print(f"\n⚠️ Models found but verification failed")

    # Create backup recommendations
    create_model_backup_script(model_paths)

    print(f"\n📋 SUMMARY:")
    if model_paths:
        print(f"✅ Found {len(model_paths)} model file(s)")
        print(f"📁 Saved in Google Drive: {BASE}")
        print(f"🎯 Main model: {MODEL_NAME}")
        print(f"🔄 Ready for loading and predictions")
    else:
        print(f"❌ No model files found")
        print(f"💡 The model might need to be retrained")
        print(f"💡 Check if training completed successfully")

if __name__ == "__main__":
    main()

#!/usr/bin/env python3
# Verify and backup your GEARS model to ensure it's properly saved

import os
import shutil
from pathlib import Path

BASE = "/content/drive/MyDrive/iPSC_CM/CM4AI_KG"
MODEL_NAME = "gears_filtered_kolf2"

def verify_model_exists():
    """
    Verify that your GEARS model exists and is complete
    """
    print("🔍 VERIFYING GEARS MODEL")
    print("=" * 25)

    os.chdir(BASE)

    model_dir = Path(MODEL_NAME)

    if model_dir.exists():
        print(f"✅ Model directory found: {model_dir}")

        # Check for required files
        required_files = ['model.pt', 'config.pkl']
        found_files = []

        for file in required_files:
            file_path = model_dir / file
            if file_path.exists():
                size = file_path.stat().st_size / (1024*1024)  # MB
                found_files.append(file)
                print(f"   ✅ {file}: {size:.1f} MB")
            else:
                print(f"   ❌ {file}: Missing")

        # Check for any additional files
        all_files = list(model_dir.iterdir())
        additional_files = [f for f in all_files if f.name not in required_files]

        if additional_files:
            print(f"   📋 Additional files:")
            for file in additional_files:
                if file.is_file():
                    size = file.stat().st_size / (1024*1024)
                    print(f"      📄 {file.name}: {size:.1f} MB")

        # Verify model completeness
        if len(found_files) >= 2:
            print(f"\n🎉 Model appears complete!")
            return True, model_dir
        else:
            print(f"\n⚠️ Model is incomplete (missing files)")
            return False, model_dir
    else:
        print(f"❌ Model directory not found: {model_dir}")
        return False, None

def test_model_loading():
    """
    Test if the model can be loaded by GEARS
    """
    print(f"\n🧪 TESTING MODEL LOADING")
    print("=" * 25)

    try:
        from gears import PertData, GEARS

        # Load dataset
        print("📊 Loading dataset...")
        pert = PertData("./data", default_pert_graph=True)
        pert.load(data_path="./data/kolf2_filtered")

        # Prepare splits and dataloaders (required for GEARS)
        pert.prepare_split(split="no_test", seed=1)
        pert.get_dataloader(batch_size=32, test_batch_size=128)

        # Create GEARS model
        device = "cuda" if os.system("nvidia-smi > /dev/null 2>&1") == 0 else "cpu"
        gears_model = GEARS(pert, device=device)

        print("🧠 Loading trained model...")
        gears_model.load_pretrained(MODEL_NAME)

        print("✅ Model loaded successfully!")

        # Test a quick prediction to verify it works
        print("🔮 Testing prediction...")
        test_prediction = gears_model.predict([['MEF2C']])  # Use a gene we know works

        if test_prediction:
            print("✅ Model prediction test successful!")
            print(f"   📊 Prediction shape: {len(test_prediction)} perturbations")
            return True, gears_model
        else:
            print("❌ Prediction test failed")
            return False, None

    except Exception as e:
        print(f"❌ Model loading failed: {e}")
        return False, None

def create_model_backup():
    """
    Create a backup of your model in multiple formats
    """
    print(f"\n💾 CREATING MODEL BACKUP")
    print("=" * 25)

    # Create backup directory
    backup_dir = Path("model_backups")
    backup_dir.mkdir(exist_ok=True)

    model_dir = Path(MODEL_NAME)

    if model_dir.exists():
        # Copy entire model directory
        backup_model_dir = backup_dir / MODEL_NAME

        if backup_model_dir.exists():
            shutil.rmtree(backup_model_dir)

        shutil.copytree(model_dir, backup_model_dir)
        print(f"✅ Model backed up to: {backup_model_dir}")

        # Create a timestamped backup
        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        timestamped_backup = backup_dir / f"{MODEL_NAME}_{timestamp}"
        shutil.copytree(model_dir, timestamped_backup)
        print(f"✅ Timestamped backup: {timestamped_backup}")

        # Create a zip archive
        zip_path = backup_dir / f"{MODEL_NAME}_backup"
        shutil.make_archive(str(zip_path), 'zip', str(model_dir))
        print(f"✅ Zip backup: {zip_path}.zip")

        return True
    else:
        print(f"❌ Model directory not found for backup")
        return False

def save_model_info():
    """
    Save model information and metadata
    """
    print(f"\n📋 SAVING MODEL INFORMATION")
    print("=" * 30)

    model_info = {
        'model_name': MODEL_NAME,
        'base_directory': BASE,
        'dataset': 'kolf2_filtered',
        'training_details': {
            'successful_perturbations': 15,
            'total_genes': 6000,
            'experiments': ['cardiac_development', 'transcription_factors',
                          'epigenetic_regulators', 'cell_cycle_control', 'metabolism_signaling']
        }
    }

    # Save as text file
    info_file = Path("model_info.txt")
    with open(info_file, 'w') as f:
        f.write(f"GEARS Model Information\n")
        f.write(f"=====================\n\n")
        f.write(f"Model Name: {model_info['model_name']}\n")
        f.write(f"Location: {model_info['base_directory']}\n")
        f.write(f"Dataset: {model_info['dataset']}\n")
        f.write(f"Successful Perturbations: {model_info['training_details']['successful_perturbations']}\n")
        f.write(f"Total Genes: {model_info['training_details']['total_genes']}\n")
        f.write(f"Experiments: {', '.join(model_info['training_details']['experiments'])}\n")
        f.write(f"\nTo load this model:\n")
        f.write(f"gears_model.load_pretrained('{MODEL_NAME}')\n")

    print(f"✅ Model info saved to: {info_file}")

    # Also save as JSON
    import json
    json_file = Path("model_info.json")
    with open(json_file, 'w') as f:
        json.dump(model_info, f, indent=2)

    print(f"✅ Model info saved to: {json_file}")

def show_usage_instructions():
    """
    Show how to use the saved model
    """
    print(f"\n📖 MODEL USAGE INSTRUCTIONS")
    print("=" * 30)

    usage_code = f"""
# Load your trained GEARS model
from gears import PertData, GEARS

# Setup dataset
pert = PertData('./data', default_pert_graph=True)
pert.load(data_path='./data/kolf2_filtered')
pert.prepare_split(split="no_test", seed=1)
pert.get_dataloader(batch_size=32, test_batch_size=128)

# Create and load model
gears_model = GEARS(pert, device='cuda')
gears_model.load_pretrained('{MODEL_NAME}')

# Make predictions
predictions = gears_model.predict([['MEF2C']])           # Single gene
predictions = gears_model.predict([['MEF2C', 'GATA4']])  # Gene combination

# Available perturbations for prediction:
working_genes = ['MEF2C', 'GATA4', 'TBX5', 'KDM6A', 'HDAC8', 'HDAC9',
                'KAT2B', 'ATR', 'AURKA', 'AURKB', 'CDKN1A', 'TPM1',
                'PPARG', 'FGF2']
"""

    print(f"```python{usage_code}```")

def main():
    """
    Main function to verify and backup your GEARS model
    """
    print("🎯 GEARS MODEL VERIFICATION & BACKUP")
    print("=" * 40)

    # Verify model exists
    model_exists, model_dir = verify_model_exists()

    if not model_exists:
        print("❌ Cannot proceed - model not found or incomplete")
        return

    # Test model loading
    model_works, loaded_model = test_model_loading()

    if not model_works:
        print("⚠️ Model exists but cannot be loaded properly")
        return

    # Create backups
    backup_success = create_model_backup()

    # Save model information
    save_model_info()

    # Show usage instructions
    show_usage_instructions()

    # Final summary
    print(f"\n🎉 MODEL VERIFICATION COMPLETE!")
    print("=" * 35)
    print(f"✅ Model exists and is complete")
    print(f"✅ Model loads and works properly")
    print(f"✅ Backups created successfully" if backup_success else "⚠️ Backup creation failed")
    print(f"✅ Usage instructions provided")

    print(f"\n📁 Your model is saved in Google Drive at:")
    print(f"   {BASE}/{MODEL_NAME}/")
    print(f"\n🚀 Ready for predictions and further research!")

if __name__ == "__main__":
    main()